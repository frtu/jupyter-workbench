{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "\n",
    "## Init Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "DataFrame[key: string, value: string]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('pyspark-jaeger').getOrCreate()\n",
    "\n",
    "#set timestamp\n",
    "spark.sql(\"set spark.sql.session.timeZone=Asia/Shanghai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- operationName: string (nullable = true)\n",
      " |-- startTime: long (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- spanID: string (nullable = true)\n",
      " |-- references: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- logs: string (nullable = true)\n",
      " |-- warnings: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df = spark.read. \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"inferSchema\", \"true\"). \\\n",
    "    option(\"delimiter\", \",\"). \\\n",
    "    option(\"escape\", \"\\\"\"). \\\n",
    "    csv(\"/data/7c1452a231ddf8a6.csv\")\n",
    "    \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- operationName: string (nullable = true)\n",
      " |-- startTime: timestamp (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- spanID: string (nullable = true)\n",
      " |-- references: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- logs: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "columns_to_drop = ['_c0', 'warnings']\n",
    "\n",
    "normDf = df. \\\n",
    "    withColumn('startTime', from_unixtime(col('startTime') / 1000000).cast(\"timestamp\")). \\\n",
    "    drop(*columns_to_drop)\n",
    "    \n",
    "normDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normDf.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normDf.groupBy(col(\"operationName\")). \\\n",
    "    count(). \\\n",
    "    orderBy(desc(\"count\")). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation & ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to convert JSON array string to a list\n",
    "def parse_json(array_str):\n",
    "    json_obj = json.loads(array_str)\n",
    "    for item in json_obj:\n",
    "       yield (item[\"key\"], item[\"type\"], item[\"value\"])\n",
    "\n",
    "def parse_all(array_str):\n",
    "    return list(parse_json(array_str.replace(\"\\'\", \"\\\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'internal.span.format', u'string', u'proto'), (u'sampler.param', u'bool', u'True'), (u'sampler.type', u'string', u'const')]"
     ]
    }
   ],
   "source": [
    "json_str = '''[{'key': 'internal.span.format', 'type': 'string', 'value': 'proto'}, {'key': 'sampler.param', 'type': 'bool', 'value': 'True'}, {'key': 'sampler.type', 'type': 'string', 'value': 'const'}]'''\n",
    "print(parse_all(json_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# https://kontext.tech/column/spark/284/pyspark-convert-json-string-column-to-array-of-object-structtype-in-data-frame\n",
    "\n",
    "# Define the schema\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "json_schema = ArrayType(StructType([StructField('key', StringType(), nullable=False), \\\n",
    "                                    StructField('type', StringType(), nullable=False), \\\n",
    "                                    StructField('value', StringType(), nullable=False)]))\n",
    "\n",
    "udf_parse_json = udf(lambda str: parse_all(str), json_schema)\n",
    "\n",
    "cleanDf = normDf.withColumn(\"tags_json\", udf_parse_json(col(\"tags\")))\n",
    "cleanDf.show(1, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
