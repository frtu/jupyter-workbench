{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "\n",
    "## Init Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pyspark-covid').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Province/State: string (nullable = true)\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      " |-- ISO 3166-1 Alpha 3-Codes: string (nullable = true)\n",
      " |-- Region Code: integer (nullable = true)\n",
      " |-- Sub-region Code: integer (nullable = true)\n",
      " |-- Intermediate Region Code: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "df = spark.read. \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"inferSchema\", \"true\"). \\\n",
    "    option(\"delimiter\", \",\"). \\\n",
    "    option(\"escape\", \"\\\"\"). \\\n",
    "    csv(\"/data/time_series_covid19_deaths_global_narrow.csv\")\n",
    "    \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize schema\n",
    "\n",
    "* Remove schema name from special char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- province_state: string (nullable = true)\n",
      " |-- country_region: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      " |-- iso_country_code: string (nullable = true)\n",
      " |-- region_code: integer (nullable = true)\n",
      " |-- sub_region_code: integer (nullable = true)\n",
      " |-- interm_region_code: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "normDf = df. \\\n",
    "    withColumnRenamed(\"ISO 3166-1 Alpha 3-Codes\", \"iso_country_code\"). \\\n",
    "    withColumnRenamed(\"Region Code\", \"region_code\"). \\\n",
    "    withColumnRenamed(\"Sub-region Code\", \"sub_region_code\"). \\\n",
    "    withColumnRenamed(\"Intermediate Region Code\", \"interm_region_code\"). \\\n",
    "    withColumnRenamed(\"Province/State\", \"province_state\"). \\\n",
    "    withColumnRenamed(\"Country/Region\", \"country_region\")\n",
    "\n",
    "normDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------+---------+---------------------+-----+----------------+-----------+---------------+------------------+\n",
      "|province_state|country_region|Lat     |Long     |Date                 |Value|iso_country_code|region_code|sub_region_code|interm_region_code|\n",
      "+--------------+--------------+--------+---------+---------------------+-----+----------------+-----------+---------------+------------------+\n",
      "|null          |Afghanistan   |33.93911|67.709953|2020-10-01 00:00:00.0|1458 |AFG             |142        |34             |null              |\n",
      "+--------------+--------------+--------+---------+---------------------+-----+----------------+-----------+---------------+------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "normDf.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Validation\n",
    "### Check overall values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|sum(sum_casualties)|\n",
      "+-------------------+\n",
      "|98961992           |\n",
      "+-------------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, col, desc\n",
    "\n",
    "column = \"sum_casualties\"\n",
    "\n",
    "isoDF = normDf. \\\n",
    "    groupBy(col(\"iso_country_code\")). \\\n",
    "    agg(sum(\"Value\").alias(column)). \\\n",
    "    orderBy(desc(column))\n",
    "\n",
    "isoDF.agg(sum(column)). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+--------+--------+------------------+\n",
      "|count(Lat)|         avg(Lat)| stddev_samp(Lat)|min(Lat)|max(Lat)|          sum(Lat)|\n",
      "+----------+-----------------+-----------------+--------+--------+------------------+\n",
      "|     67564|21.07662424812158|24.85792699104551|-51.7963| 71.7069|1424021.0407000864|\n",
      "+----------+-----------------+-----------------+--------+--------+------------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, mean, stddev, min, max, sum\n",
    "\n",
    "normDf.agg(count(\"Lat\"), mean(\"Lat\"), stddev(\"Lat\"), min(\"Lat\"), max(\"Lat\"), sum(\"Lat\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "distinctValuesDF = normDf. \\\n",
    "    select(col(\"iso_country_code\"), col(\"country_region\"), col(\"province_state\"), col(\"Lat\"), col(\"Long\")). \\\n",
    "    distinct()\n",
    "\n",
    "print(distinctValuesDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|iso_country_code|count|\n",
      "+----------------+-----+\n",
      "|             CHN|   31|\n",
      "|             CAN|   12|\n",
      "|             AUS|    8|\n",
      "|            null|    5|\n",
      "|             SPM|    1|\n",
      "|             FRA|    1|\n",
      "|             POL|    1|\n",
      "|             TCA|    1|\n",
      "|             LVA|    1|\n",
      "|             JAM|    1|\n",
      "|             ZMB|    1|\n",
      "|             BRA|    1|\n",
      "|             ARM|    1|\n",
      "|             MOZ|    1|\n",
      "|             JOR|    1|\n",
      "|             CUB|    1|\n",
      "|             ABW|    1|\n",
      "|             SOM|    1|\n",
      "|             BRN|    1|\n",
      "|             COD|    1|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "distinctValuesDF.groupBy(\"iso_country_code\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------------+-------+-------+\n",
      "|iso_country_code|  country_region|  province_state|    Lat|   Long|\n",
      "+----------------+----------------+----------------+-------+-------+\n",
      "|            null|  United Kingdom| Channel Islands|49.3723|-2.3644|\n",
      "|            null|      MS Zaandam|            null|    0.0|    0.0|\n",
      "|            null|Diamond Princess|            null|    0.0|    0.0|\n",
      "|            null|          Canada|Diamond Princess|    0.0|    0.0|\n",
      "|            null|          Canada|  Grand Princess|    0.0|    0.0|\n",
      "+----------------+----------------+----------------+-------+-------+"
     ]
    }
   ],
   "source": [
    "distinctValuesDF.where(\"iso_country_code is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+--------------+------------------+--------+\n",
      "|iso_country_code|country_region|province_state|               Lat|    Long|\n",
      "+----------------+--------------+--------------+------------------+--------+\n",
      "|             CHN|         China|      Xinjiang|           41.1129| 85.2401|\n",
      "|             CHN|         China|        Shanxi|           37.5777|112.2922|\n",
      "|             CHN|         China|       Guangxi|           23.8298|108.7881|\n",
      "|             CHN|         China|         Hebei|            39.549|116.1306|\n",
      "|             CHN|         China|      Shanghai|31.201999999999998|121.4491|\n",
      "|             CHN|         China|         Hunan|           27.6104|111.7088|\n",
      "|             CHN|         China|      Shandong|           36.3427|118.1498|\n",
      "|             CHN|         China|       Beijing|           40.1824|116.4142|\n",
      "|             CHN|         China|       Shaanxi|           35.1917|108.8701|\n",
      "|             CHN|         China|         Tibet|           31.6927| 88.0924|\n",
      "|             CHN|         China|       Jiangxi|            27.614|115.7221|\n",
      "|             CHN|         China|         Hubei|           30.9756|112.2707|\n",
      "|             CHN|         China|       Guizhou|           26.8154|106.8748|\n",
      "|             CHN|         China|         Anhui|           31.8257|117.2264|\n",
      "|             CHN|         China|  Heilongjiang|47.861999999999995|127.7615|\n",
      "|             CHN|         China|        Fujian|           26.0789|117.9874|\n",
      "|             CHN|         China|         Gansu|           35.7518|104.2861|\n",
      "|             CHN|         China|        Hainan|           19.1959|109.7453|\n",
      "|             CHN|         China|         Henan|           37.8957|114.9042|\n",
      "|             CHN|         China|       Ningxia|           37.2692|106.1655|\n",
      "+----------------+--------------+--------------+------------------+--------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "distinctValuesDF.where(col(\"iso_country_code\") == \"CHN\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|start_date_range     |end_date_range       |\n",
      "+---------------------+---------------------+\n",
      "|2020-01-22 00:00:00.0|2020-10-01 00:00:00.0|\n",
      "+---------------------+---------------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "column = \"Date\"\n",
    "normDf.agg(min(column).alias(\"start_date_range\"), max(column).alias(\"end_date_range\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand distribution\n",
    "### Parent child columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|iso_country_code|count|\n",
      "+----------------+-----+\n",
      "|             HTI|    1|\n",
      "|             PSE|    1|\n",
      "|             POL|    1|\n",
      "|             BRB|    1|\n",
      "|             LVA|    1|\n",
      "|             JAM|    1|\n",
      "|             ZMB|    1|\n",
      "|             SPM|    1|\n",
      "|             BRA|    1|\n",
      "|             ARM|    1|\n",
      "|             MOZ|    1|\n",
      "|             JOR|    1|\n",
      "|             CUB|    1|\n",
      "|             ABW|    1|\n",
      "|             SOM|    1|\n",
      "|             FRA|    1|\n",
      "|             TCA|    1|\n",
      "|             BRN|    1|\n",
      "|             COD|    1|\n",
      "|             URY|    1|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "distinctValuesDF.groupBy(col(\"iso_country_code\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|iso_country_code|count|\n",
      "+----------------+-----+\n",
      "|             CHN|   31|\n",
      "|             CAN|   12|\n",
      "|             AUS|    8|\n",
      "|            null|    5|\n",
      "|             SPM|    1|\n",
      "|             FRA|    1|\n",
      "|             POL|    1|\n",
      "|             TCA|    1|\n",
      "|             LVA|    1|\n",
      "|             JAM|    1|\n",
      "|             ZMB|    1|\n",
      "|             BRA|    1|\n",
      "|             ARM|    1|\n",
      "|             MOZ|    1|\n",
      "|             JOR|    1|\n",
      "|             CUB|    1|\n",
      "|             ABW|    1|\n",
      "|             SOM|    1|\n",
      "|             BRN|    1|\n",
      "|             COD|    1|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "distinctValuesDF.groupBy(col(\"iso_country_code\")). \\\n",
    "    count(). \\\n",
    "    orderBy(desc(\"count\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|country_region|count|contained_province                                                                                                                                                                                                                                                                                        |\n",
      "+--------------+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Australia     |8    |[Queensland, New South Wales, Northern Territory, Western Australia, Australian Capital Territory, South Australia, Victoria, Tasmania]                                                                                                                                                                   |\n",
      "|Canada        |14   |[Northwest Territories, Manitoba, Newfoundland and Labrador, Grand Princess, New Brunswick, Alberta, Prince Edward Island, Ontario, Nova Scotia, Diamond Princess, Yukon, Quebec, British Columbia, Saskatchewan]                                                                                         |\n",
      "|China         |33   |[Inner Mongolia, Jilin, Fujian, Yunnan, Heilongjiang, Hebei, Hong Kong, Hubei, Tianjin, Liaoning, Hunan, Chongqing, Ningxia, Shanghai, Guangdong, Zhejiang, Shandong, Guangxi, Qinghai, Macau, Tibet, Anhui, Shaanxi, Hainan, Guizhou, Gansu, Jiangsu, Beijing, Jiangxi, Sichuan, Xinjiang, Shanxi, Henan]|\n",
      "|Denmark       |2    |[Faroe Islands, Greenland]                                                                                                                                                                                                                                                                                |\n",
      "|France        |10   |[Martinique, French Polynesia, Reunion, French Guiana, New Caledonia, Guadeloupe, Saint Pierre and Miquelon, St Martin, Mayotte, Saint Barthelemy]                                                                                                                                                        |\n",
      "|Netherlands   |4    |[Bonaire, Sint Eustatius and Saba, Aruba, Sint Maarten, Curacao]                                                                                                                                                                                                                                          |\n",
      "|United Kingdom|10   |[Gibraltar, Channel Islands, Isle of Man, Montserrat, Anguilla, Turks and Caicos Islands, Bermuda, British Virgin Islands, Cayman Islands, Falkland Islands (Malvinas)]                                                                                                                                   |\n",
      "+--------------+-----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, collect_set\n",
    "\n",
    "distinctValuesDF.groupBy(col(\"country_region\")). \\\n",
    "    agg(count(\"province_state\").alias(\"count\"), collect_set(\"province_state\").alias(\"contained_province\")). \\\n",
    "    where(col(\"count\") > 0). \\\n",
    "    orderBy(\"country_region\"). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution By Time Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+--------------------------------+---------------------------------------------+----------+\n",
      "|iso_country_code|country_region        |province_state                  |window                                       |casualties|\n",
      "+----------------+----------------------+--------------------------------+---------------------------------------------+----------+\n",
      "|BIH             |Bosnia and Herzegovina|null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|861       |\n",
      "|SVK             |Slovakia              |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|48        |\n",
      "|ITA             |Italy                 |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|35918     |\n",
      "|MMR             |Burma                 |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|321       |\n",
      "|SGP             |Singapore             |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|27        |\n",
      "|CIV             |Cote d'Ivoire         |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|120       |\n",
      "|CZE             |Czechia               |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|678       |\n",
      "|GRC             |Greece                |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|393       |\n",
      "|MCO             |Monaco                |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|2         |\n",
      "|MYS             |Malaysia              |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|136       |\n",
      "|VEN             |Venezuela             |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|635       |\n",
      "|RWA             |Rwanda                |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|29        |\n",
      "|TUR             |Turkey                |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|8262      |\n",
      "|CHN             |China                 |Guangdong                       |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|8         |\n",
      "|HTI             |Haiti                 |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|229       |\n",
      "|GIN             |Guinea                |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|66        |\n",
      "|BES             |Netherlands           |Bonaire, Sint Eustatius and Saba|[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|1         |\n",
      "|BRN             |Brunei                |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|3         |\n",
      "|FRO             |Denmark               |Faroe Islands                   |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|0         |\n",
      "|BRA             |Brazil                |null                            |[2020-10-01 00:00:00.0,2020-10-02 00:00:00.0]|144680    |\n",
      "+----------------+----------------------+--------------------------------+---------------------------------------------+----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, desc, window\n",
    "\n",
    "tsDF = normDf. \\\n",
    "    groupBy(col(\"iso_country_code\"), col(\"country_region\"), col(\"province_state\"), window(col(\"Date\"), \"1 day\")). \\\n",
    "    agg(sum(\"Value\").alias(\"casualties\")). \\\n",
    "    orderBy(desc(\"window.start\"))\n",
    "\n",
    "tsDF. \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window value stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|start_date_range     |end_date_range       |\n",
      "+---------------------+---------------------+\n",
      "|2020-01-22 00:00:00.0|2020-10-01 00:00:00.0|\n",
      "+---------------------+---------------------+"
     ]
    }
   ],
   "source": [
    "column = \"window.start\"\n",
    "tsDF.agg(min(column).alias(\"start_date_range\"), max(column).alias(\"end_date_range\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation & ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+--------------+---------------------+---------------------+----------+\n",
      "|iso_country_code|country_region|province_state|start                |end                  |casualties|\n",
      "+----------------+--------------+--------------+---------------------+---------------------+----------+\n",
      "|null            |MS Zaandam    |unknown       |2020-01-22 00:00:00.0|2020-01-23 00:00:00.0|0         |\n",
      "+----------------+--------------+--------------+---------------------+---------------------+----------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "storedDF = tsDF.select(\"iso_country_code\", \"country_region\", \"province_state\", \"window.start\", \"window.end\", \"casualties\"). \\\n",
    "    na.fill(\"unknown\", subset=[\"country_region\", \"province_state\"]). \\\n",
    "    orderBy(\"iso_country_code\", \"start\")\n",
    "\n",
    "storedDF.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storedDF.coalesce(1). \\\n",
    "    write.mode('overwrite'). \\\n",
    "    option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\"). \\\n",
    "    option(\"header\",\"true\"). \\\n",
    "    csv(\"/data/covid-19-by-country.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data into Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storedDF. \\\n",
    "    write.mode(\"overwrite\"). \\\n",
    "    saveAsTable(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataframe using .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandaDf = tsDF.select(\"iso_country_code\", \"window.start\", \"casualties\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandaDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name=u'data', database=u'default', description=None, tableType=u'MANAGED', isTemporary=False)]"
     ]
    }
   ],
   "source": [
    "spark.catalog.listTables(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL - Data query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0592c2ecf460482a95020848655deee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), stâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef33acd580b1423c8b244211669aaf8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdf = spark.sql(\"SELECT start as time, iso_country_code, casualties \\\n",
    "FROM data \\\n",
    "WHERE iso_country_code = 'FRA' \\\n",
    "ORDER BY time\") \\\n",
    "pdf.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install --user matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(pdf['time'], pdf['casualties'], color='red')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for country in pdf['iso_country_code']:\n",
    "    dataframe = pdf.loc[pdf['iso_country_code'] == country]\n",
    "    ax.plot(dataframe['time'], dataframe['casualties'], label=country)\n",
    "\n",
    "ax.set_title('Casualties per country')\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "\n",
    "sns.catplot(x ='time', y ='casualties', data = pdf)\n",
    "plt.title('COVID-19 Casualities')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
