{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "\n",
    "## Init Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pyspark-covid').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read. \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"inferSchema\", \"true\"). \\\n",
    "    option(\"delimiter\", \",\"). \\\n",
    "    option(\"escape\", \"\\\"\"). \\\n",
    "    csv(\"/data/time_series_covid19_deaths_global_narrow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "### Type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDf = df. \\\n",
    "    withColumnRenamed(\"ISO 3166-1 Alpha 3-Codes\", \"iso_country_code\"). \\\n",
    "    withColumnRenamed(\"Region Code\", \"region_code\"). \\\n",
    "    withColumnRenamed(\"Sub-region Code\", \"sub_region_code\"). \\\n",
    "    withColumnRenamed(\"Intermediate Region Code\", \"interm_region_code\"). \\\n",
    "    withColumnRenamed(\"Province/State\", \"province_state\"). \\\n",
    "    withColumnRenamed(\"Country/Region\", \"country_region\")\n",
    "\n",
    "cleanDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation\n",
    "### Check overall values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, col, desc\n",
    "\n",
    "column = \"sum_casualties\"\n",
    "\n",
    "isoDF = cleanDf. \\\n",
    "    groupBy(col(\"iso_country_code\")). \\\n",
    "    agg(sum(\"Value\").alias(column)). \\\n",
    "    orderBy(desc(column))\n",
    "\n",
    "isoDF.agg(sum(column)). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, mean, stddev, min, max, sum\n",
    "\n",
    "df.agg(count(\"Lat\"), mean(\"Lat\"), stddev(\"Lat\"), min(\"Lat\"), max(\"Lat\"), sum(\"Lat\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "distinctValuesDF = cleanDf. \\\n",
    "    select(col(\"iso_country_code\"), col(\"country_region\"), col(\"province_state\"), col(\"Lat\"), col(\"Long\")). \\\n",
    "    distinct()\n",
    "\n",
    "print(distinctValuesDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "distinctValuesDF.groupBy(\"iso_country_code\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctValuesDF.where(\"iso_country_code is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "distinctValuesDF.where(col(\"iso_country_code\") == \"CHN\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "column = \"Date\"\n",
    "df.agg(min(column).alias(\"start_date_range\"), max(column).alias(\"end_date_range\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand distribution\n",
    "### Parent child columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctValuesDF.groupBy(col(\"iso_country_code\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctValuesDF.groupBy(col(\"iso_country_code\")). \\\n",
    "    count(). \\\n",
    "    orderBy(desc(\"count\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, collect_set\n",
    "\n",
    "distinctValuesDF.groupBy(col(\"country_region\")). \\\n",
    "    agg(count(\"province_state\").alias(\"count\"), collect_set(\"province_state\").alias(\"contained_province\")). \\\n",
    "    where(col(\"count\") > 0). \\\n",
    "    orderBy(\"country_region\"). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution By Time Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, desc, window\n",
    "\n",
    "tsDF = cleanDf. \\\n",
    "    groupBy(col(\"iso_country_code\"), col(\"country_region\"), col(\"province_state\"), window(col(\"Date\"), \"1 day\")). \\\n",
    "    agg(sum(\"Value\").alias(\"casualties\")). \\\n",
    "    orderBy(desc(\"window.start\"))\n",
    "\n",
    "tsDF. \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window value stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \"window.start\"\n",
    "tsDF.agg(min(column).alias(\"start_date_range\"), max(column).alias(\"end_date_range\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation & ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storedDF = tsDF.select(\"iso_country_code\", \"country_region\", \"province_state\", \"window.start\", \"window.end\", \"casualties\"). \\\n",
    "    na.fill(\"unknown\", subset=[\"country_region\", \"province_state\"]). \\\n",
    "    orderBy(\"iso_country_code\", \"start\")\n",
    "\n",
    "storedDF.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storedDF.coalesce(1). \\\n",
    "    write.mode('overwrite'). \\\n",
    "    option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\"). \\\n",
    "    option(\"header\",\"true\"). \\\n",
    "    csv(\"/data/covid-19.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data into Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "storedDF. \\\n",
    "    write.mode(\"overwrite\"). \\\n",
    "    saveAsTable(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataframe to %%local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandaDf = tsDF.select(\"iso_country_code\", \"window.start\", \"casualties\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandaDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(pdf['time'], pdf['casualties'], color='red')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "fig, ax = plt.subplots()\n",
    "for country in pdf['iso_country_code']:\n",
    "    dataframe = pdf.loc[pdf['iso_country_code'] == country]\n",
    "    ax.plot(dataframe['time'], dataframe['casualties'], label=country)\n",
    "\n",
    "ax.set_title('Casualties per country')\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "\n",
    "sns.catplot(x ='time', y ='casualties', data = pdf)\n",
    "plt.title('COVID-19 Casualities')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
