{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "\n",
    "## Init Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('pyspark-covid').getOrCreate()\n",
    "#set timestamp\n",
    "spark.sql(\"set spark.sql.session.timeZone=UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Province/State: string (nullable = true)\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      " |-- ISO 3166-1 Alpha 3-Codes: string (nullable = true)\n",
      " |-- Region Code: integer (nullable = true)\n",
      " |-- Sub-region Code: integer (nullable = true)\n",
      " |-- Intermediate Region Code: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read. \\\n",
    "    option(\"header\", \"true\"). \\\n",
    "    option(\"inferSchema\", \"true\"). \\\n",
    "    option(\"delimiter\", \",\"). \\\n",
    "    option(\"escape\", \"\\\"\"). \\\n",
    "    csv(\"/data/time_series_covid19_deaths_global_narrow.csv\")\n",
    "    \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize schema\n",
    "\n",
    "* Remove schema name from special char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- province_state: string (nullable = true)\n",
      " |-- country_region: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      " |-- iso_country_code: string (nullable = true)\n",
      " |-- region_code: integer (nullable = true)\n",
      " |-- sub_region_code: integer (nullable = true)\n",
      " |-- interm_region_code: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normDf = df. \\\n",
    "    withColumnRenamed(\"ISO 3166-1 Alpha 3-Codes\", \"iso_country_code\"). \\\n",
    "    withColumnRenamed(\"Region Code\", \"region_code\"). \\\n",
    "    withColumnRenamed(\"Sub-region Code\", \"sub_region_code\"). \\\n",
    "    withColumnRenamed(\"Intermediate Region Code\", \"interm_region_code\"). \\\n",
    "    withColumnRenamed(\"Province/State\", \"province_state\"). \\\n",
    "    withColumnRenamed(\"Country/Region\", \"country_region\")\n",
    "\n",
    "normDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------+---------+----------+-----+----------------+-----------+---------------+------------------+\n",
      "|province_state|country_region|Lat     |Long     |Date      |Value|iso_country_code|region_code|sub_region_code|interm_region_code|\n",
      "+--------------+--------------+--------+---------+----------+-----+----------------+-----------+---------------+------------------+\n",
      "|null          |Afghanistan   |33.93911|67.709953|2020-10-01|1458 |AFG             |142        |34             |null              |\n",
      "+--------------+--------------+--------+---------+----------+-----+----------------+-----------+---------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normDf.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data type or value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date & Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- province_state: string (nullable = true)\n",
      " |-- country_region: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      " |-- iso_country_code: string (nullable = true)\n",
      " |-- region_code: integer (nullable = true)\n",
      " |-- sub_region_code: integer (nullable = true)\n",
      " |-- interm_region_code: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normDf = normDf. \\\n",
    "    withColumn(\"Date\", col(\"Date\").cast(TimestampType()))\n",
    "\n",
    "normDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test date parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+\n",
      "|@timestamp                 |date               |\n",
      "+---------------------------+-------------------+\n",
      "|May 15th 2021, 12:34:56.789|2021-05-15 12:34:56|\n",
      "+---------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "date_df = spark.createDataFrame([(1,'May 15th 2021, 12:34:56.789')], ['id','@timestamp'])\n",
    "\n",
    "# https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html\n",
    "date_df.select(\n",
    "    '@timestamp', \n",
    "    from_unixtime(unix_timestamp('@timestamp', \"MMM dd'th' yyyy, HH:mm:ss.SSS\")).alias('date')\n",
    ").show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF - Custom transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert value : AFG\n",
      "afg\n"
     ]
    }
   ],
   "source": [
    "def convert(str, debug=False):\n",
    "    if (debug):\n",
    "        print(\"convert value :\", str)\n",
    "    if str is None:\n",
    "        return None\n",
    "    return str.lower() \n",
    "\n",
    "convertUDF = udf(lambda str: convert(str), StringType())\n",
    "\n",
    "print(convert(\"AFG\", True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------+---------+----------+-----+----------------+-----------+---------------+------------------+\n",
      "|province_state|country_region|Lat     |Long     |Date      |Value|iso_country_code|region_code|sub_region_code|interm_region_code|\n",
      "+--------------+--------------+--------+---------+----------+-----+----------------+-----------+---------------+------------------+\n",
      "|null          |Afghanistan   |33.93911|67.709953|2020-10-01|1458 |afg             |142        |34             |null              |\n",
      "+--------------+--------------+--------+---------+----------+-----+----------------+-----------+---------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normDf. \\\n",
    "    withColumn(\"iso_country_code\", convertUDF(col(\"iso_country_code\"))). \\\n",
    "    show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert value : Logger text: PARAM1, PARAM2 - PARAM3\n",
      "{'k1': 'PARAM1', 'k2': 'PARAM2', 'k3': 'PARAM3'}\n",
      "convert value : Non matching text\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import re\n",
    "\n",
    "def convert(str, debug=False):\n",
    "    if (debug):\n",
    "        print(\"convert value :\", str)\n",
    "    if str is None:\n",
    "        return None\n",
    "\n",
    "    searchResult = re.search(r'Logger text: (\\w+), (\\w+) - (\\w+)', str)\n",
    "    if searchResult:\n",
    "        result = dict()\n",
    "        result['k1'], result['k2'], result['k3'] = searchResult.groups()\n",
    "        return result\n",
    "    else:\n",
    "        return None \n",
    "\n",
    "convertUDF = udf(convert, MapType(StringType(), StringType()))\n",
    "\n",
    "print(convert('Logger text: PARAM1, PARAM2 - PARAM3', True))\n",
    "print(convert('Non matching text', True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|k1 |k2 |k3 |\n",
      "+---+---+---+\n",
      "|V1 |V2 |V3 |\n",
      "|W1 |W2 |W3 |\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logList = [\"Logger text: V1, V2 - V3\", \"Logger text: W1, W2 - W3\"]\n",
    "logDF = spark.createDataFrame(logList, StringType())\n",
    "\n",
    "\n",
    "convertDF = logDF.select(convertUDF(\"value\").alias(\"log\"))\n",
    "convertDF.select(col(\"log.k1\"), col(\"log.k2\"), col(\"log.k3\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|   V1|\n",
      "|   W1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regexp_extract -> Extract one particular text\n",
    "result = logDF.withColumn('value', regexp_extract(col('value'), 'Logger text: (\\w+), (\\w+) - (\\w+)', 1))\n",
    "result.select('value').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert Timestamp value : 2021-Jan-15 12:34\n",
      "2021-01-15 04:34:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def convertTimestamp(str, debug=False):\n",
    "    if (debug):\n",
    "        print(\"convert Timestamp value :\", str)\n",
    "    if str is None:\n",
    "        return None\n",
    "    # https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\n",
    "    return (datetime.strptime(str,'%Y-%b-%d %H:%M') - timedelta(hours=8))\n",
    "\n",
    "convertTimestampUDF = udf(lambda str: convertTimestamp(str), TimestampType())\n",
    "\n",
    "print(convertTimestamp('2021-Jan-15 12:34', True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert value : 1\n",
      "k1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import re\n",
    "\n",
    "mapper = dict()\n",
    "mapper['1'] = 'k1'\n",
    "mapper['2'] = 'k2'\n",
    "mapper['3'] = 'k3'\n",
    "\n",
    "def mapping(str, debug=False):\n",
    "    if (debug):\n",
    "        print(\"convert value :\", str)\n",
    "    if str is None:\n",
    "        return None\n",
    "    \n",
    "    return mapper[str]\n",
    "\n",
    "mappingUDF = udf(mapping, StringType())\n",
    "\n",
    "print(mapping('1', True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet Validation\n",
    "### Check overall values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|sum(sum_casualties)|\n",
      "+-------------------+\n",
      "|98961992           |\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, col, desc\n",
    "\n",
    "column = \"sum_casualties\"\n",
    "\n",
    "isoDF = normDf. \\\n",
    "    groupBy(col(\"iso_country_code\")). \\\n",
    "    agg(sum(\"Value\").alias(column)). \\\n",
    "    orderBy(desc(column))\n",
    "\n",
    "isoDF.agg(sum(column)). \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+--------+--------+------------------+\n",
      "|count(Lat)|         avg(Lat)| stddev_samp(Lat)|min(Lat)|max(Lat)|          sum(Lat)|\n",
      "+----------+-----------------+-----------------+--------+--------+------------------+\n",
      "|     67564|21.07662424812158|24.85792699104551|-51.7963| 71.7069|1424021.0407000864|\n",
      "+----------+-----------------+-----------------+--------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, mean, stddev, min, max, sum\n",
    "\n",
    "normDf.agg(count(\"Lat\"), mean(\"Lat\"), stddev(\"Lat\"), min(\"Lat\"), max(\"Lat\"), sum(\"Lat\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "distinctValuesDF = normDf. \\\n",
    "    select(col(\"iso_country_code\"), col(\"country_region\"), col(\"province_state\"), col(\"Lat\"), col(\"Long\")). \\\n",
    "    distinct()\n",
    "\n",
    "print(distinctValuesDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|iso_country_code|count|\n",
      "+----------------+-----+\n",
      "|             CHN|   31|\n",
      "|             CAN|   12|\n",
      "|             AUS|    8|\n",
      "|            null|    5|\n",
      "|             SPM|    1|\n",
      "|             ABW|    1|\n",
      "|             BRB|    1|\n",
      "|             COD|    1|\n",
      "|             LVA|    1|\n",
      "|             HTI|    1|\n",
      "|             PSE|    1|\n",
      "|             BRA|    1|\n",
      "|             ARM|    1|\n",
      "|             JOR|    1|\n",
      "|             CUB|    1|\n",
      "|             MOZ|    1|\n",
      "|             SOM|    1|\n",
      "|             FRA|    1|\n",
      "|             BRN|    1|\n",
      "|             TCA|    1|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "distinctValuesDF.groupBy(\"iso_country_code\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------------+-------+-------+\n",
      "|iso_country_code|  country_region|  province_state|    Lat|   Long|\n",
      "+----------------+----------------+----------------+-------+-------+\n",
      "|            null|          Canada|  Grand Princess|    0.0|    0.0|\n",
      "|            null|      MS Zaandam|            null|    0.0|    0.0|\n",
      "|            null|          Canada|Diamond Princess|    0.0|    0.0|\n",
      "|            null|  United Kingdom| Channel Islands|49.3723|-2.3644|\n",
      "|            null|Diamond Princess|            null|    0.0|    0.0|\n",
      "+----------------+----------------+----------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinctValuesDF.where(\"iso_country_code is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+--------------+------------------+------------------+\n",
      "|iso_country_code|country_region|province_state|               Lat|              Long|\n",
      "+----------------+--------------+--------------+------------------+------------------+\n",
      "|             CHN|         China|       Qinghai|           35.7452|           95.9956|\n",
      "|             CHN|         China|       Ningxia|           37.2692|          106.1655|\n",
      "|             CHN|         China|      Xinjiang|           41.1129|           85.2401|\n",
      "|             CHN|         China|      Shandong|           36.3427|          118.1498|\n",
      "|             CHN|         China|       Guangxi|           23.8298|          108.7881|\n",
      "|             CHN|         China|         Gansu|           35.7518|          104.2861|\n",
      "|             CHN|         China|       Jiangsu|           32.9711|           119.455|\n",
      "|             CHN|         China|         Henan|           37.8957|          114.9042|\n",
      "|             CHN|         China|         Tibet|           31.6927|           88.0924|\n",
      "|             CHN|         China|      Shanghai|31.201999999999998|          121.4491|\n",
      "|             CHN|         China|        Yunnan|            24.974|101.48700000000001|\n",
      "|             CHN|         China|        Fujian|           26.0789|          117.9874|\n",
      "|             CHN|         China|       Sichuan|           30.6171|          102.7103|\n",
      "|             CHN|         China|         Hebei|            39.549|          116.1306|\n",
      "|             CHN|         China|       Shaanxi|           35.1917|          108.8701|\n",
      "|             CHN|         China|Inner Mongolia|           44.0935|          113.9448|\n",
      "|             CHN|         China|         Jilin|           43.6661|          126.1923|\n",
      "|             CHN|         China|         Hunan|           27.6104|          111.7088|\n",
      "|             CHN|         China|     Chongqing|           30.0572|           107.874|\n",
      "|             CHN|         China|         Hubei|           30.9756|          112.2707|\n",
      "+----------------+--------------+--------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "distinctValuesDF.where(col(\"iso_country_code\") == \"CHN\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n",
      "|start_date_range|end_date_range|\n",
      "+----------------+--------------+\n",
      "|2020-01-22      |2020-10-01    |\n",
      "+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "column = \"Date\"\n",
    "normDf.agg(min(column).alias(\"start_date_range\"), max(column).alias(\"end_date_range\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand distribution\n",
    "### Parent child columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|iso_country_code|count|\n",
      "+----------------+-----+\n",
      "|             HTI|    1|\n",
      "|             PSE|    1|\n",
      "|             BRB|    1|\n",
      "|             POL|    1|\n",
      "|             LVA|    1|\n",
      "|             ZMB|    1|\n",
      "|             JAM|    1|\n",
      "|             SPM|    1|\n",
      "|             BRA|    1|\n",
      "|             ARM|    1|\n",
      "|             MOZ|    1|\n",
      "|             JOR|    1|\n",
      "|             CUB|    1|\n",
      "|             SOM|    1|\n",
      "|             FRA|    1|\n",
      "|             ABW|    1|\n",
      "|             TCA|    1|\n",
      "|             COD|    1|\n",
      "|             BRN|    1|\n",
      "|             BOL|    1|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinctValuesDF.groupBy(col(\"iso_country_code\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|iso_country_code|count|\n",
      "+----------------+-----+\n",
      "|             CHN|   31|\n",
      "|             CAN|   12|\n",
      "|             AUS|    8|\n",
      "|            null|    5|\n",
      "|             SPM|    1|\n",
      "|             ABW|    1|\n",
      "|             LVA|    1|\n",
      "|             COD|    1|\n",
      "|             PSE|    1|\n",
      "|             ZMB|    1|\n",
      "|             JAM|    1|\n",
      "|             BRA|    1|\n",
      "|             ARM|    1|\n",
      "|             MOZ|    1|\n",
      "|             JOR|    1|\n",
      "|             CUB|    1|\n",
      "|             SOM|    1|\n",
      "|             FRA|    1|\n",
      "|             BRN|    1|\n",
      "|             TCA|    1|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinctValuesDF.groupBy(col(\"iso_country_code\")). \\\n",
    "    count(). \\\n",
    "    orderBy(desc(\"count\")). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|iso_country_code|country_region|count|value_list                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |contained_province                                                                                                                                                                                                                                                                      |\n",
      "+----------------+--------------+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CHN             |China         |31   |[0.8385826771653543, 0.0, 1.484251968503937, 2.47244094488189, 0.0, 5.688976377952756, 2.7755905511811023, 5.547244094488189, 20.0, 3.645669291338583, 2.645669291338583, 7.090551181102362, 0.8858267716535433, 1.688976377952756, 0.0, 1.8425196850393701, 6.161417322834645, 1.8622047244094488, 5.5236220472440944, 1.8385826771653544, 0.0, 5.496062992125984, 0.9251968503937008, 3739.0708661417325, 0.8858267716535433, 5.468503937007874, 12.023622047244094, 1.7755905511811023, 7.783464566929134, 2.779527559055118, 0.0]|[Inner Mongolia, Jilin, Fujian, Yunnan, Heilongjiang, Hebei, Hubei, Tianjin, Liaoning, Hunan, Chongqing, Ningxia, Shanghai, Guangdong, Zhejiang, Shandong, Qinghai, Guangxi, Tibet, Anhui, Shaanxi, Hainan, Guizhou, Gansu, Jiangsu, Beijing, Jiangxi, Sichuan, Xinjiang, Shanxi, Henan]|\n",
      "|CAN             |Canada        |12   |[2.1141732283464565, 0.003937007874015748, 116.96456692913385, 1634.1889763779527, 3203.196850393701, 38.01968503937008, 120.1259842519685, 0.9094488188976378, 10.248031496062993, 6.389763779527559, 0.0, 0.0]                                                                                                                                                                                                                                                                                                                     |[Northwest Territories, Newfoundland and Labrador, Manitoba, New Brunswick, Yukon, Quebec, British Columbia, Alberta, Prince Edward Island, Ontario, Nova Scotia, Saskatchewan]                                                                                                         |\n",
      "|AUS             |Australia     |8    |[4.291338582677166, 141.49606299212599, 33.32283464566929, 8.606299212598426, 6.2952755905511815, 2.1141732283464565, 0.0, 2.7716535433070866]                                                                                                                                                                                                                                                                                                                                                                                       |[Queensland, New South Wales, Northern Territory, Victoria, Western Australia, Australian Capital Territory, South Australia, Tasmania]                                                                                                                                                 |\n",
      "+----------------+--------------+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, collect_set\n",
    "\n",
    "regionDf = normDf.where(\"province_state IS NOT NULL\"). \\\n",
    "    groupBy(\"iso_country_code\", \"country_region\", \"province_state\"). \\\n",
    "        agg(avg(\"Value\").alias(\"avg_val\")).\\\n",
    "    groupBy(\"iso_country_code\", \"country_region\"). \\\n",
    "        agg( \\\n",
    "            count(\"province_state\").alias(\"count\"), \\\n",
    "            # Collect all values\n",
    "            collect_list(\"avg_val\").alias(\"value_list\"), \\\n",
    "            # Collect all distinct values\n",
    "            collect_set(\"province_state\").alias(\"contained_province\") \\\n",
    "        ). \\\n",
    "    where(col(\"count\") > 1). \\\n",
    "    where(\"iso_country_code IS NOT NULL\"). \\\n",
    "    orderBy(desc(\"count\"))\n",
    "\n",
    "regionDf. \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution By Time Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+--------------------------------+------------------------------------------+----------+\n",
      "|iso_country_code|country_region        |province_state                  |window                                    |casualties|\n",
      "+----------------+----------------------+--------------------------------+------------------------------------------+----------+\n",
      "|SGP             |Singapore             |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|27        |\n",
      "|ITA             |Italy                 |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|35918     |\n",
      "|MYS             |Malaysia              |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|136       |\n",
      "|MMR             |Burma                 |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|321       |\n",
      "|CZE             |Czechia               |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|678       |\n",
      "|BIH             |Bosnia and Herzegovina|null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|861       |\n",
      "|CHN             |China                 |Guizhou                         |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|2         |\n",
      "|BRN             |Brunei                |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|3         |\n",
      "|GRC             |Greece                |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|393       |\n",
      "|SVK             |Slovakia              |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|48        |\n",
      "|VEN             |Venezuela             |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|635       |\n",
      "|FRO             |Denmark               |Faroe Islands                   |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|0         |\n",
      "|BES             |Netherlands           |Bonaire, Sint Eustatius and Saba|[2020-10-01 00:00:00, 2020-10-02 00:00:00]|1         |\n",
      "|BRA             |Brazil                |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|144680    |\n",
      "|RWA             |Rwanda                |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|29        |\n",
      "|TUR             |Turkey                |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|8262      |\n",
      "|GIN             |Guinea                |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|66        |\n",
      "|MCO             |Monaco                |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|2         |\n",
      "|CIV             |Cote d'Ivoire         |null                            |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|120       |\n",
      "|CHN             |China                 |Guangdong                       |[2020-10-01 00:00:00, 2020-10-02 00:00:00]|8         |\n",
      "+----------------+----------------------+--------------------------------+------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, desc, window\n",
    "\n",
    "tsDF = normDf. \\\n",
    "    groupBy(col(\"iso_country_code\"), col(\"country_region\"), col(\"province_state\"), window(col(\"Date\"), \"1 day\")). \\\n",
    "    agg(sum(\"Value\").alias(\"casualties\")). \\\n",
    "    orderBy(desc(\"window.start\"))\n",
    "\n",
    "tsDF. \\\n",
    "    show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window value stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|start_date_range   |end_date_range     |\n",
      "+-------------------+-------------------+\n",
      "|2020-01-22 00:00:00|2020-10-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column = \"window.start\"\n",
    "tsDF.agg(min(column).alias(\"start_date_range\"), max(column).alias(\"end_date_range\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation & ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+--------------+-------------------+-------------------+----------+\n",
      "|iso_country_code|country_region|province_state|start              |end                |casualties|\n",
      "+----------------+--------------+--------------+-------------------+-------------------+----------+\n",
      "|null            |Canada        |Grand Princess|2020-01-22 00:00:00|2020-01-23 00:00:00|0         |\n",
      "+----------------+--------------+--------------+-------------------+-------------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "storedDF = tsDF.select(\"iso_country_code\", \"country_region\", \"province_state\", \"window.start\", \"window.end\", \"casualties\"). \\\n",
    "    na.fill(\"unknown\", subset=[\"country_region\", \"province_state\"]). \\\n",
    "    orderBy(\"iso_country_code\", \"start\")\n",
    "\n",
    "storedDF.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "storedDF.coalesce(1). \\\n",
    "    write.mode('overwrite'). \\\n",
    "    option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\"). \\\n",
    "    option(\"header\",\"true\"). \\\n",
    "    csv(\"/data/covid-19-by-country.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data into Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "storedDF. \\\n",
    "    write.mode(\"overwrite\"). \\\n",
    "    saveAsTable(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataframe using .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandaDf = regionDf.select(\"count\", \"country_region\", \"contained_province\", \"value_list\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandaDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='data', database='default', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install --user matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso_country_code</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USA</td>\n",
       "      <td>23125806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRA</td>\n",
       "      <td>11794882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBR</td>\n",
       "      <td>6521458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ITA</td>\n",
       "      <td>6049186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEX</td>\n",
       "      <td>5889714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IND</td>\n",
       "      <td>5411897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FRA</td>\n",
       "      <td>5062399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ESP</td>\n",
       "      <td>4976185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PER</td>\n",
       "      <td>2442735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IRN</td>\n",
       "      <td>2432434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RUS</td>\n",
       "      <td>1744841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BEL</td>\n",
       "      <td>1618427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DEU</td>\n",
       "      <td>1485093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>COL</td>\n",
       "      <td>1465253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CAN</td>\n",
       "      <td>1303569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CHL</td>\n",
       "      <td>1058022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ZAF</td>\n",
       "      <td>1057560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NLD</td>\n",
       "      <td>1031336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iso_country_code     Total\n",
       "0               USA  23125806\n",
       "1               BRA  11794882\n",
       "2               GBR   6521458\n",
       "3               ITA   6049186\n",
       "4               MEX   5889714\n",
       "5               IND   5411897\n",
       "6               FRA   5062399\n",
       "7               ESP   4976185\n",
       "8               PER   2442735\n",
       "9               IRN   2432434\n",
       "10              RUS   1744841\n",
       "11              BEL   1618427\n",
       "12              DEU   1485093\n",
       "13              COL   1465253\n",
       "14              CAN   1303569\n",
       "15              CHL   1058022\n",
       "16              ZAF   1057560\n",
       "17              NLD   1031336"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = spark.sql(\"SELECT iso_country_code, sum(casualties) as Total \\\n",
    "FROM data_table \\\n",
    "GROUP BY iso_country_code \\\n",
    "ORDER BY Total DESC\").where(\"Total > 1000000\").toPandas()\n",
    "\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Visualization\n",
    "fig = plt.pie(pdf['Total'], autopct='%1.1f%%', startangle=140,labels=pdf['iso_country_code'])\n",
    "plt.title('Casulaties per country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>iso_country_code</th>\n",
       "      <th>casualties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>FRA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>FRA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>FRA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>FRA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>FRA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2020-09-27</td>\n",
       "      <td>FRA</td>\n",
       "      <td>31483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2020-09-28</td>\n",
       "      <td>FRA</td>\n",
       "      <td>31549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>2020-09-29</td>\n",
       "      <td>FRA</td>\n",
       "      <td>31711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>FRA</td>\n",
       "      <td>31769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>FRA</td>\n",
       "      <td>31816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          time iso_country_code  casualties\n",
       "0   2020-01-22              FRA           0\n",
       "1   2020-01-23              FRA           0\n",
       "2   2020-01-24              FRA           0\n",
       "3   2020-01-25              FRA           0\n",
       "4   2020-01-26              FRA           0\n",
       "..         ...              ...         ...\n",
       "249 2020-09-27              FRA       31483\n",
       "250 2020-09-28              FRA       31549\n",
       "251 2020-09-29              FRA       31711\n",
       "252 2020-09-30              FRA       31769\n",
       "253 2020-10-01              FRA       31816\n",
       "\n",
       "[254 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = spark.sql(\"SELECT start as time, iso_country_code, casualties \\\n",
    "FROM data \\\n",
    "WHERE iso_country_code = 'FRA' \\\n",
    "ORDER BY time\").toPandas()\n",
    "\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pdf['time'], pdf['casualties'], color='red')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiline plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for country in pdf['iso_country_code']:\n",
    "    dataframe = pdf.loc[pdf['iso_country_code'] == country]\n",
    "    ax.plot(dataframe['time'], dataframe['casualties'], label=country)\n",
    "\n",
    "ax.set_title('Casualties per country')\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pandaDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-c39f29f82d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpandaDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country_region'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandaDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpandaDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country_region'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value_list'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pandaDf' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for key in pandaDf['country_region']:\n",
    "    dataframe = pandaDf.loc[pandaDf['country_region'] == key]\n",
    "    values = pd.Series(dataframe['value_list'].tolist()[0])\n",
    "    print(values.shape)\n",
    "    data.insert(loc=0, column=key, value=values)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "for (columnName, columnData) in data.iteritems(): \n",
    "    plt.figure()\n",
    "    print('Colunm Name : ', columnName) \n",
    "    print('Column Contents : ', type(columnData))\n",
    "    columnData.plot(kind='box')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "\n",
    "sns.catplot(x ='time', y ='casualties', data = pdf)\n",
    "plt.title('COVID-19 Casualities')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
