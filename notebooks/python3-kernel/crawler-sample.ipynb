{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys  \n",
    "!{sys.executable} -m pip install --user requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Crawler : crawler for one full website\n",
    "# import Downloader: Remote download & storing to local storage\n",
    "%run /notebooks/python3-kernel/libs/crawler-class.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "## Initialize crawler\n",
    "\n",
    "* Turn `debug=True` passing it as last parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://xxx\"\n",
    "\n",
    "crawler = Crawler(BASE_URL)\n",
    "soup = crawler.parseUrl(BASE_URL)\n",
    "\n",
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter HTML tag and normalize names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagenameFromTag(tag):\n",
    "    return pagename(tag.text.strip())\n",
    "\n",
    "def pagename(fullname):\n",
    "    splitted_names = str(fullname).split(\"-\", 1)\n",
    "    name = splitted_names[1]\n",
    "    name = name.strip().replace(\" \", \"-\")\n",
    "    return name.strip()\n",
    "\n",
    "pagenameFromTag(soup.title)\n",
    "pagename(\"Free Download GIFs - Get the best GIF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Downloader\n",
    "\n",
    "Downloader help to :\n",
    "\n",
    "* retrieve content\n",
    "* organize & store them in local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH=\"/data/crawler/\" + pagenameFromTag(soup.title)\n",
    "\n",
    "downloader = Downloader(BASE_PATH)\n",
    "downloader.getRootFolder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl & Download content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all URL found for 1 page\n",
    "page_url = BASE_URL\n",
    "capture_section = \"index.php?section=\"\n",
    "capture_page = \"page.php?id=\"\n",
    "\n",
    "soup = crawler.loadUrls(page_url, \"a\", \"class\", \"^MenuLink-sc-1ftqwtj\", None, True)\n",
    "soup = crawler.loadUrlsFromLinks(page_url, \"a\", \"href\", capture_section, True)\n",
    "print(crawler.getUniqueUrls())\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(full_url, base_url, debug=False):\n",
    "    regex = \"^\" + re.escape(base_url) + \"(.+)$\"\n",
    "    name_grp = re.search(regex, full_url)\n",
    "    if (debug):\n",
    "        print(\"Search regex:\", regex, \" response:\", name_grp)\n",
    "    if name_grp is not None:\n",
    "        return name_grp.group(1)\n",
    "\n",
    "extract_name(\"/base/section/abcd\", \"/base/page/\", True) # Should not raise error\n",
    "extract_name(\"/base/page/id_1234\", \"/base/page/\", True)\n",
    "\n",
    "extract_name(\"index.php?section=1\", capture_section, True) # Should not raise error\n",
    "extract_name(\"page.php?id=1234\", capture_page, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_url(*paths):\n",
    "    return BASE_URL + \"\".join(paths)\n",
    "\n",
    "build_url(capture_section, \"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all URL found for each subsite\n",
    "for section_url in crawler.getUniqueUrls():\n",
    "    # Aim to downlaod image from subpage\n",
    "    crawlerPage = Crawler(section_url)\n",
    "    soup = crawlerPage.parseUrl(build_url(section_url))\n",
    "\n",
    "    if Url(section_url) != Url(BASE_URL):\n",
    "        section = extract_name(section_url, capture_section)\n",
    "        crawlerPage.loadUrlsFromLinks(build_url(section_url), \"a\", \"href\", capture_page)\n",
    "        print(\"== Found section:\", section, \" unique_urls:\", crawlerPage.getUniqueUrls())\n",
    "\n",
    "        for page_url in crawlerPage.getUniqueUrls():\n",
    "            item = extract_name(page_url, capture_page)\n",
    "            if item is not None:\n",
    "                if section is None:\n",
    "                    path = item\n",
    "                else:\n",
    "                    path = item + \"(\" + section + \")\"\n",
    "                print(\"* Load path:\", path)\n",
    "                if not downloader.isExist(path):\n",
    "                    soup = crawler.loadUrls(build_url(page_url), \"a\", \"href\", \"^\" + capture_page)\n",
    "                    downloader.downloadAllTags(path, soup, \"img\", \"class\", \"gif-img\", \"href\", True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
