{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede3d13b-18d5-4c62-9aa2-94523f4e6ab2",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "Has all the local BERT & integration with ES. TBD :\n",
    "\n",
    "* using the new `dense_vector` format\n",
    "* tied with the query using search(.., knn=..)\n",
    "* can use [search_template](https://www.elastic.co/docs/solutions/search/search-templates)\n",
    "* create an unbox func for hits hits\n",
    "\n",
    "See other notebook for more : `Semantic - ElasticSearch with OpenAI.ipynb`\n",
    "\n",
    "# Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663bf85f-0a86-4a50-8da5-964219efbdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceef8b5-c0c4-4c9c-ab50-b48922b13b9e",
   "metadata": {},
   "source": [
    "## Init base libs\n",
    "\n",
    "* Install libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03cfae-e3f7-4450-9677-ecfa5074c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \\\n",
    "    python-dotenv \\\n",
    "    pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c22f5a-bdb2-4b9f-a3f8-fa7ae6f62c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.os import OsUtil\n",
    "\n",
    "os_util = OsUtil()\n",
    "result = os_util.get_env(\"PATH\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb00ad-0282-4512-bd41-08502b2f2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def read_yaml_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads YAML file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "    \n",
    "# config = read_yaml_file(\"./config.yml\")\n",
    "# config[\"cloud_id\"], api_key=config[\"api_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980d59d-6174-49c3-a753-c2d2c6543a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads and loads a JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40166136-ff0e-49b2-a1d6-cdc3c4592199",
   "metadata": {},
   "source": [
    "## Init ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a016a10-b423-44f2-acbf-dab7417c57e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \\\n",
    "    elasticsearch==8.15.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191d661-a3bd-42b2-a922-010b379a2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.elasticsearch import ElasticsearchClient\n",
    "\n",
    "es_client = ElasticsearchClient()\n",
    "print(\"Ping:\", es_client.ping())\n",
    "print(\"ES Info:\", es_client.info()) # should return cluster info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f9c1e-da65-407f-9dd6-0bf676ec6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_index(index_name: str, body):\n",
    "    es = es_client.get_client()\n",
    "    response = es.search(index=index_name, body=body)\n",
    "    return response\n",
    "\n",
    "def search_knn_index(index_name: str, field_name:str, query):\n",
    "    es = es_client.get_client()\n",
    "    query = {\n",
    "        \"field\": field_name,\n",
    "        \"query_vector\": query,\n",
    "        \"knn\": 2,\n",
    "        \"num_candidates\": 500\n",
    "    }\n",
    "    # , source=[]\n",
    "    response = es.knn_search(index=index_name, knn=query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91585eec-02f1-4d73-a755-c2fcb04e29eb",
   "metadata": {},
   "source": [
    "## Init Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca6dc9-ab17-42ab-9749-5b6c16160b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \\\n",
    "    transformers \\\n",
    "    sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46227ffa-7f56-45c1-9f7d-9a7b53664837",
   "metadata": {},
   "source": [
    "### Semantic Search\n",
    "\n",
    "#### Sentence transformer\n",
    "\n",
    "* Select any [model](https://sbert.net/docs/sentence_transformer/pretrained_models.html#original-models) by performance\n",
    "* https://sbert.net/docs/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16067727-4a7f-4109-9b0f-fd066aefcdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", device_type=\"cpu\"):\n",
    "        # 1. Load a pretrained Sentence Transformer model\n",
    "        self.model = SentenceTransformer(model_name, device=device_type)\n",
    "\n",
    "    def get_text_vector(self, sentences:list):\n",
    "        # 2. Calculate embeddings by calling model.encode()\n",
    "        sentence_embeddings = self.model.encode(sentences)\n",
    "        return sentence_embeddings\n",
    "\n",
    "    def get_tokens(self, documents) -> list :\n",
    "        sentences = [documents]\n",
    "        return list(get_text_vector(sentences).flatten())\n",
    "\n",
    "def get_text_vector(sentences:list, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", device_type=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Generates sentence embeddings using pre-trained model.\n",
    "    \"\"\"\n",
    "    return Tokenizer().get_text_vector(sentences)\n",
    "\n",
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "embeddings = get_text_vector(sentences)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86350c71-8665-4f54-ae75-9b5d761c90b0",
   "metadata": {},
   "source": [
    "## Loading dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d3b033-7c35-435c-9fc0-ce3a6eae8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \\\n",
    "    opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15074d-6bcb-4e70-9532-6aeaa5d3193c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import opendatasets as od \n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/madhab/jobposts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8fe25d-c905-4405-89cc-0dc5d796976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('jobposts/data job posts.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a878b-edf5-4142-a332-d0e0150bfc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sampleDf = df.iloc[np.random.permutation(len(df))]\n",
    "sampleDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094994d2-1dac-4cad-b207-29ea7c11e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(sampleDf.head(500)[\"Title\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb23d57-40a2-441e-9149-bee408b24a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "sentence_embedding = tokenizer.get_tokens(\"Software Engineer\")\n",
    "sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2b3d1-bb21-4f1f-9db7-31251da63c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.head(5)\n",
    "dataset['vector'] = dataset['jobpost'].apply(tokenizer.get_tokens)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd1ac7-2c3e-4cfb-bdee-e7249a0fabd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(sentence_embeddings):\n",
    "    encoded_np_array = np.array(sentence_embeddings)\n",
    "    encoded_list = encoded_np_array.tolist()\n",
    "    return encoded_list\n",
    "\n",
    "dataset['vector'] = dataset['vector'].apply(wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b1769-ea98-4f9e-8039-5a15d6cedaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search-api.html\n",
    "\n",
    "index_name = \"job-posts\"\n",
    "mapping = {\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"number_of_shards\" :20,\n",
    "      \"number_of_replicas\": 1,\n",
    "      \"knn\":{\n",
    "        \"algo_param\":{\n",
    "          \"ef_search\":40,\n",
    "          \"ef_construction\":40,\n",
    "          \"m\": \"4\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"knn\": \"true\"\n",
    "  }, \n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"id\": {\"type\": \"keyword\"},\n",
    "      \"jobposts\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"vector\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 384\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "es_client.create_index(index_name, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10575ad3-dcaf-45ca-b0a0-51bfa98e815b",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "\n",
    "https://huggingface.co/docs/datasets/en/quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15074d-6bcb-4e70-9532-6aeaa5d3193c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \\\n",
    "    datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b1ae5-7aa1-4abd-9492-b4013e8e2f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# https://huggingface.co/datasets/quora\n",
    "dataset = load_dataset('quora')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb094c-652d-43c6-a372-73c5d41c793c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "\n",
    "for record in dataset['questions']:\n",
    "    questions.extend(record['text'])\n",
    "\n",
    "questions = list(set(questions))\n",
    "print('\\n'.join(questions[:3]))\n",
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a3c6b-90b1-4e7f-8f44-4a73bf54da8c",
   "metadata": {},
   "source": [
    "# Semantic Search\n",
    "\n",
    "## Sentence transformer\n",
    "\n",
    "https://sbert.net/docs/quickstart.html\n",
    "\n",
    "* max_seq_length : max # of tokens encoded into a single vector embedding. Beyond is truncated\n",
    "* word_embedding_dimension : # of dimensionality of vector\n",
    "* Normalize : final step is normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59bcf63-22bd-4d3f-8e8e-50ca61fd65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "embeddings = get_text_vector(sentences)\n",
    "print(embeddings.shape)\n",
    "# [3, 384]\n",
    "\n",
    "# => Calculate the embedding similarities\n",
    "# similarities = model.similarity(embeddings, embeddings)\n",
    "# print(similarities)\n",
    "# tensor([[1.0000, 0.6660, 0.1046],\n",
    "#         [0.6660, 1.0000, 0.1411],\n",
    "#         [0.1046, 0.1411, 1.0000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fea98d-4a35-49f1-94b9-35b745bfc2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
