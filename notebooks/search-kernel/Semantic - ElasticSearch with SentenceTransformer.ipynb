{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede3d13b-18d5-4c62-9aa2-94523f4e6ab2",
   "metadata": {},
   "source": [
    "# Prepare environment\n",
    "\n",
    "## Init base libs\n",
    "\n",
    "* Install libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03cfae-e3f7-4450-9677-ecfa5074c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \\\n",
    "    python-dotenv \\\n",
    "    pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c22f5a-bdb2-4b9f-a3f8-fa7ae6f62c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb00ad-0282-4512-bd41-08502b2f2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def read_yaml_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads YAML file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "    \n",
    "# config = read_yaml_file(\"./config.yml\")\n",
    "# config[\"cloud_id\"], api_key=config[\"api_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980d59d-6174-49c3-a753-c2d2c6543a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads and loads a JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40166136-ff0e-49b2-a1d6-cdc3c4592199",
   "metadata": {},
   "source": [
    "## Init ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a016a10-b423-44f2-acbf-dab7417c57e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \\\n",
    "    elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191d661-a3bd-42b2-a922-010b379a2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "def get_client_es(hosts:str=\"http://elasticsearch:9200\", max_retries:int=5):\n",
    "    \"\"\"\n",
    "    Initializes Elasticsearch client using cloud_id and api_key from config.yml\n",
    "    \"\"\"\n",
    "    es = Elasticsearch(hosts=hosts)\n",
    "    return es.options(max_retries=max_retries)\n",
    "\n",
    "es = get_client_es()\n",
    "es.info() # should return cluster info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e21cf7-ff3a-4904-be53-b28629c2b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.elastic.co/search-labs/tutorials/search-tutorial/full-text-search/create-index\n",
    "def create_index(index_name: str, mappings):\n",
    "    es = get_client_es()\n",
    "    if not es.indices.exists(index=index_name):\n",
    "        response = es.indices.create(index=index_name, body=mappings)\n",
    "        if response.meta.status != 200:\n",
    "            raise RuntimeError(\"failed to create index\")\n",
    "        print(f\"Index '{index_name}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Index '{index_name}' already exists.\")\n",
    "        response = es.indices.get(index=index_name)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d9936b-77cf-4b7b-b8aa-82c5059a5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_index(index_name: str) -> int:\n",
    "    es = get_client_es()\n",
    "    count = int(es.cat.count(index=index_name, format=\"json\")[0][\"count\"])\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f9c1e-da65-407f-9dd6-0bf676ec6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_index(index_name: str, body):\n",
    "    es = get_client_es()\n",
    "    response = es.search(index=index_name, body=body)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91585eec-02f1-4d73-a755-c2fcb04e29eb",
   "metadata": {},
   "source": [
    "## Init Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca6dc9-ab17-42ab-9749-5b6c16160b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \\\n",
    "    transformers \\\n",
    "    sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46227ffa-7f56-45c1-9f7d-9a7b53664837",
   "metadata": {},
   "source": [
    "### Semantic Search\n",
    "\n",
    "#### Sentence transformer\n",
    "\n",
    "https://sbert.net/docs/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16067727-4a7f-4109-9b0f-fd066aefcdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_text_vector(sentences, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", device_type=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Generates sentence embeddings using pre-trained model.\n",
    "    \"\"\"\n",
    "    # 1. Load a pretrained Sentence Transformer model\n",
    "    model = SentenceTransformer(model_name, device=device_type)\n",
    "    # 2. Calculate embeddings by calling model.encode()\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings\n",
    "\n",
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "embeddings = get_text_vector(sentences)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86350c71-8665-4f54-ae75-9b5d761c90b0",
   "metadata": {},
   "source": [
    "## Loading dataset\n",
    "\n",
    "https://huggingface.co/docs/datasets/en/quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15074d-6bcb-4e70-9532-6aeaa5d3193c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \\\n",
    "    datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093b1ae5-7aa1-4abd-9492-b4013e8e2f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# https://huggingface.co/datasets/quora\n",
    "dataset = load_dataset('quora')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb094c-652d-43c6-a372-73c5d41c793c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "\n",
    "for record in dataset['questions']:\n",
    "    questions.extend(record['text'])\n",
    "\n",
    "questions = list(set(questions))\n",
    "print('\\n'.join(questions[:3]))\n",
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a3c6b-90b1-4e7f-8f44-4a73bf54da8c",
   "metadata": {},
   "source": [
    "# Semantic Search\n",
    "\n",
    "## Sentence transformer\n",
    "\n",
    "https://sbert.net/docs/quickstart.html\n",
    "\n",
    "* max_seq_length : max # of tokens encoded into a single vector embedding. Beyond is truncated\n",
    "* word_embedding_dimension : # of dimensionality of vector\n",
    "* Normalize : final step is normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59bcf63-22bd-4d3f-8e8e-50ca61fd65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "embeddings = get_text_vector(sentences)\n",
    "print(embeddings.shape)\n",
    "# [3, 384]\n",
    "\n",
    "# => Calculate the embedding similarities\n",
    "# similarities = model.similarity(embeddings, embeddings)\n",
    "# print(similarities)\n",
    "# tensor([[1.0000, 0.6660, 0.1046],\n",
    "#         [0.6660, 1.0000, 0.1411],\n",
    "#         [0.1046, 0.1411, 1.0000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fea98d-4a35-49f1-94b9-35b745bfc2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
