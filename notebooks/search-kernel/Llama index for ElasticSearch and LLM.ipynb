{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede3d13b-18d5-4c62-9aa2-94523f4e6ab2",
   "metadata": {},
   "source": [
    "# Prepare environment\n",
    "\n",
    "## Configure jupyter\n",
    "\n",
    "* Install libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03cfae-e3f7-4450-9677-ecfa5074c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "%pip install python-dotenv\n",
    "%pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c22f5a-bdb2-4b9f-a3f8-fa7ae6f62c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setting properties\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ[\"PINECONE_ENV\"] = os.getenv(\"PINECONE_ENV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91585eec-02f1-4d73-a755-c2fcb04e29eb",
   "metadata": {},
   "source": [
    "## Init Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca6dc9-ab17-42ab-9749-5b6c16160b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "%pip install transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7327a20-8c2d-4fb3-8859-644076ccccc8",
   "metadata": {},
   "source": [
    "## Init Llama index\n",
    "\n",
    "* https://docs.llamaindex.ai/en/stable/understanding/using_llms/using_llms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ee18f-f005-4762-bfb9-67dc63698d3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "%pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7069ec8-0e62-43ba-9706-8f729d81cb44",
   "metadata": {},
   "source": [
    "### LLM Choices\n",
    "\n",
    "#### For Ollama\n",
    "\n",
    "* https://docs.llamaindex.ai/en/stable/examples/embeddings/ollama_embedding/\n",
    "\n",
    "##### Ollama startup & verify\n",
    "\n",
    "* https://replicate.com/blog/run-llama-locally\n",
    "\n",
    "Start & check Ollama using\n",
    "\n",
    "* http://localhost:11434/v1/chat/completions\n",
    "\n",
    "```\n",
    "{\n",
    "  \"model\": \"mistral:instruct\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Hello!\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc4f2d1-6036-4177-b522-c28301496026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "%pip install ollama\n",
    "%pip install llama-index-llms-ollama\n",
    "%pip install llama-index-embeddings-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0c2e2-0396-4572-95a8-d0ee7ea46942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from here, run ollama\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "llm_model = \"mistral\"\n",
    "llm_url = \"http://localhost:11434\"\n",
    "\n",
    "# Embedding Model to do local embedding using Ollama.\n",
    "embedding = OllamaEmbedding(\n",
    "    model_name=llm_model,\n",
    "    base_url=llm_url,\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "response = embedding.get_text_embedding_batch(\n",
    "    [\"This is a passage!\", \"This is another passage\"], show_progress=True\n",
    ")\n",
    "# print(response)\n",
    "query_embedding = embedding.get_query_embedding(\"Where is blue?\")\n",
    "# print(query_embedding)\n",
    "\n",
    "# Completion\n",
    "llm = Ollama(model=llm_model, base_url=llm_url, request_timeout=60.0)\n",
    "response = llm.complete(\"Who is Laurie Voss? write in 10 words\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3168eb-f4bd-48e8-b772-185d8a33a182",
   "metadata": {},
   "source": [
    "#### LLM - For OpenAI\n",
    "\n",
    "* https://docs.llamaindex.ai/en/stable/examples/llm/openai/\n",
    "* https://llamahub.ai/l/llms/llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3247a2-14f6-4103-8c57-fb548a0b744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "\n",
    "# https://platform.openai.com/docs/libraries\n",
    "%pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6aa7fa-70ea-4926-9d7c-b63671d29ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai  # for calling the OpenAI API\n",
    "\n",
    "# Load your API key from an environment variable or secret management service\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635dac2e-2325-4f13-bd08-7c2e2ad2f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69f19e-b2b7-4d0a-9d66-7a4d88e494b7",
   "metadata": {},
   "source": [
    "### Vector stores\n",
    "\n",
    "#### For ElasticSearch\n",
    "\n",
    "* https://docs.llamaindex.ai/en/stable/examples/vector_stores/ElasticsearchIndexDemo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4c52d-0cd2-44c2-b66c-64916bac9341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "%pip install llama-index-vector-stores-elasticsearch elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8a3af-1231-41fc-bba9-70b15f3c5ead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.elasticsearch import ElasticsearchStore\n",
    "\n",
    "# VectorStore that takes care of ES Index and Data management.\n",
    "vector_store = ElasticsearchStore(index_name=\"conversation\",\n",
    "                 es_url=\"http://localhost:9200\",\n",
    "                 # es_cloud_id=os.getenv(\"ELASTIC_CLOUD_ID\"),\n",
    "                 # es_api_key=os.getenv(\"ELASTIC_API_KEY\")\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b257f-5790-497f-9d03-3ff50662ddf9",
   "metadata": {},
   "source": [
    "### Rerank\n",
    "\n",
    "#### For Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e4065-31b1-46df-a100-86f9def3b4d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "%pip install cohere llama-index-postprocessor-cohere-rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c577327d-e9ea-4b0c-8355-7428a5225544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ec936-d354-472f-9d68-3e359c328762",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef645cc-1abd-4c56-b316-b401e78fe49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p '/data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O '/data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0222d867-f252-4b67-9edf-313440288038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cdb7c1-1490-40c1-8a22-eacf9342ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize without metadata filter\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e303b61-cda4-4508-a3ff-abf746b0488e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
