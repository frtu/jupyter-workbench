{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkMagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th>Magic</th>\n",
       "    <th>Example</th>\n",
       "    <th>Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>info</td>\n",
       "    <td>%%info</td>\n",
       "    <td>Outputs session information for the current Livy endpoint.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>cleanup</td>\n",
       "    <td>%%cleanup -f</td>\n",
       "    <td>Deletes all sessions for the current Livy endpoint, including this notebook's session. The force flag is mandatory.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>delete</td>\n",
       "    <td>%%delete -f -s 0</td>\n",
       "    <td>Deletes a session by number for the current Livy endpoint. Cannot delete this kernel's session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>logs</td>\n",
       "    <td>%%logs</td>\n",
       "    <td>Outputs the current session's Livy logs.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure</td>\n",
       "    <td>%%configure -f<br/>{\"executorMemory\": \"1000M\", \"executorCores\": 4}</td>\n",
       "    <td>Configure the session creation parameters. The force flag is mandatory if a session has already been\n",
       "    created and the session will be dropped and recreated.<br/>Look at <a href=\"https://github.com/cloudera/livy#request-body\">\n",
       "    Livy's POST /sessions Request Body</a> for a list of valid parameters. Parameters must be passed in as a JSON string.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>spark</td>\n",
       "    <td>%%spark -o df<br/>df = spark.read.parquet('...</td>\n",
       "    <td>Executes spark commands.\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The Spark dataframe of name VAR_NAME will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe with the same name.</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a dataframe that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>sql</td>\n",
       "    <td>%%sql -o tables -q<br/>SHOW TABLES</td>\n",
       "    <td>Executes a SQL query against the variable sqlContext (Spark v1.x) or spark (Spark v2.x).\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The result of the SQL query will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe.</li>\n",
       "        <li>-q: The magic will return None instead of the dataframe (no visualization).</li>\n",
       "        <li>-m, -n, -r are the same as the %%spark parameters above.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>local</td>\n",
       "    <td>%%local<br/>a = 1</td>\n",
       "    <td>All the code in subsequent lines will be executed locally. Code must be valid Python code.</td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detailed command, please see https://docs.faculty.ai/how_to/spark/external_cluster.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%configure -f \n",
    "{\n",
    "    \"name\":\"sparkmagic-demo\",\n",
    "    \"kind\": \"spark\",\n",
    "    \"executorMemory\": \"4G\", \n",
    "    \"executorCores\": 4,\n",
    "    \"driverMemory\": \"1000M\", \n",
    "    \"numExecutors\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "res2: String = 2.1.0"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "// https://spark.apache.org/docs/2.1.0/sql-programming-guide.html\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res4: String = in-memory"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.catalogImplementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.driver.host,172.18.0.2)\n",
      "(spark.livy.spark_major_version,2)\n",
      "(spark.driver.port,34723)\n",
      "(hive.metastore.warehouse.dir,file:/usr/spark-2.1.0/spark-warehouse/)\n",
      "(spark.repl.class.uri,spark://172.18.0.2:34723/classes)\n",
      "(spark.jars,file:/apps/livy-server-0.3.0/rsc-jars/livy-rsc-0.3.0.jar,file:/apps/livy-server-0.3.0/rsc-jars/netty-all-4.0.29.Final.jar,file:/apps/livy-server-0.3.0/rsc-jars/unused-1.0.0.jar,file:/apps/livy-server-0.3.0/rsc-jars/spark-tags_2.10-2.1.0.jar,file:/apps/livy-server-0.3.0/rsc-jars/livy-api-0.3.0.jar,file:/apps/livy-server-0.3.0/repl_2.11-jars/commons-codec-1.9.jar,file:/apps/livy-server-0.3.0/repl_2.11-jars/livy-repl_2.11-0.3.0.jar,file:/apps/livy-server-0.3.0/repl_2.11-jars/livy-core_2.11-0.3.0.jar)\n",
      "(spark.repl.class.outputDir,/tmp/spark398781943972724046)\n",
      "(spark.app.name,livy-session-2)\n",
      "(spark.driver.memory,1000M)\n",
      "(spark.executor.id,driver)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.yarn.maxAppAttempts,1)\n",
      "(spark.master,local)\n",
      "(spark.yarn.submit.waitAppCompletion,false)\n",
      "(spark.sql.catalogImplementation,in-memory)\n",
      "(spark.executor.cores,2)\n",
      "(spark.app.id,local-1601911973526)"
     ]
    }
   ],
   "source": [
    "spark.conf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.master,local)\n",
      "(spark.sql.catalogImplementation,in-memory)\n",
      "(spark.driver.port,34723)\n",
      "(spark.yarn.submit.waitAppCompletion,false)\n",
      "(spark.executor.id,driver)\n",
      "(spark.executor.cores,2)\n",
      "(spark.repl.class.uri,spark://172.18.0.2:34723/classes)\n",
      "(spark.app.id,local-1601911973526)\n",
      "(spark.driver.host,172.18.0.2)\n",
      "(spark.repl.class.outputDir,/tmp/spark398781943972724046)\n",
      "(spark.yarn.maxAppAttempts,1)\n",
      "(spark.livy.spark_major_version,2)\n",
      "(spark.app.name,livy-session-2)\n",
      "(spark.submit.deployMode,client)\n",
      "(hive.metastore.warehouse.dir,file:/usr/spark-2.1.0/spark-warehouse/)\n",
      "(spark.jars,file:/apps/livy-server-0.3.0/rsc-jars/livy-rsc-0.3.0.jar,file:/apps/livy-server-0.3.0/rsc-jars/netty-all-4.0.29.Final.jar,file:/apps/livy-server-0.3.0/rsc-jars/unused-1.0.0.jar,file:/apps/livy-server-0.3.0/rsc-jars/spark-tags_2.10-2.1.0.jar,file:/apps/livy-server-0.3.0/rsc-jars/livy-api-0.3.0.jar,file:/apps/livy-server-0.3.0/repl_2.11-jars/commons-codec-1.9.jar,file:/apps/livy-server-0.3.0/repl_2.11-jars/livy-repl_2.11-0.3.0.jar,file:/apps/livy-server-0.3.0/repl_2.11-jars/livy-core_2.11-0.3.0.jar)\n",
      "(spark.driver.memory,1000M)"
     ]
    }
   ],
   "source": [
    "println(sc.version)\n",
    "sc.getConf.getAll.\n",
    "    foreach(println)\n",
    "//     mkString(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Dataset\n",
    "import spark.sqlContext.implicits._\n",
    "\n",
    "val jsonInCSV: Dataset[String] = sc.parallelize(List(\n",
    "  \"\"\"\n",
    "    |\"a\",\"b\",\"c\",\"{\"\"x\"\":\"\"xx\"\",\"\"y\"\":\"\"yy\"\"}\"\n",
    "  \"\"\".stripMargin)).toDS()\n",
    "\n",
    "val df = spark.read.option(\"escape\", \"\\\"\").csv(jsonInCSV)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res10: String = 1978-08-19T19:08:08.788Z"
     ]
    }
   ],
   "source": [
    "val value = \"[\\\"1978-08-19T19:08:08.788Z\\\",\\\"2020-08-19T19:08:08.788Z\\\"]\"\n",
    "value.substring(2, value.indexOf(',')-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res13: Long = 1514764800000"
     ]
    }
   ],
   "source": [
    "import java.text.SimpleDateFormat\n",
    "\n",
    "val pattern = \"dd/MM/yyyy HH:mm\"\n",
    "\n",
    "val format = new SimpleDateFormat(pattern)\n",
    "format.parse(\"01/01/2018 00:00\").getTime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stringTime: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)"
     ]
    }
   ],
   "source": [
    "import java.text.SimpleDateFormat\n",
    "\n",
    "def parseDateUDF(p: String) = udf(\n",
    "  (value: String) => {\n",
    "    val dateFormat = new SimpleDateFormat(p)\n",
    "    val parsedDate = dateFormat.parse(value)\n",
    "    new java.sql.Timestamp(parsedDate.getTime())\n",
    "  }\n",
    ")\n",
    "\n",
    "val pattern = \"dd/MM/yyyy HH:mm\"\n",
    "\n",
    "// Example data\n",
    "val df = Seq(\n",
    "  Tuple1(\"01/01/2018 00:00\")\n",
    ").toDF(\"stringTime\")\n",
    "\n",
    "val newDF = df.withColumn(\"timestamp\", parseDateUDF(pattern)(df(\"stringTime\"))).orderBy(\"timestamp\")\n",
    "newDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [Province/State: string, Country/Region: string ... 8 more fields]"
     ]
    }
   ],
   "source": [
    "// http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframereader#pyspark.sql.DataFrameReader.csv\n",
    "\n",
    "val isoDatePattern = \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"\n",
    "val datePattern = \"yyyy-MM-dd HH:mm:ss.SSS\"\n",
    "\n",
    "val df = spark.read.\n",
    "    option(\"header\", \"true\").\n",
    "    option(\"inferSchema\", \"true\").\n",
    "    option(\"delimiter\", \",\").\n",
    "// Use this line to allow detect Date type\n",
    "//     option(\"timestampFormat\", datePattern).\n",
    "// Use this line if one row can have line break\n",
    "//     option(\"multiline\",true).\n",
    "// Use this line if one column has JSON confuse with delimiter \n",
    "//     option(\"escape\", \"\\\"\").\n",
    "    csv(\"file:///data/time_series_covid19_deaths_global_narrow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Province/State: string (nullable = true)\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      " |-- ISO 3166-1 Alpha 3-Codes: string (nullable = true)\n",
      " |-- Region Code: integer (nullable = true)\n",
      " |-- Sub-region Code: integer (nullable = true)\n",
      " |-- Intermediate Region Code: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------+---------+---------------------+-----+------------------------+-----------+---------------+------------------------+\n",
      "|Province/State|Country/Region|Lat     |Long     |Date                 |Value|ISO 3166-1 Alpha 3-Codes|Region Code|Sub-region Code|Intermediate Region Code|\n",
      "+--------------+--------------+--------+---------+---------------------+-----+------------------------+-----------+---------------+------------------------+\n",
      "|null          |Afghanistan   |33.93911|67.709953|2020-10-01 00:00:00.0|1458 |AFG                     |142        |34             |null                    |\n",
      "+--------------+--------------+--------+---------+---------------------+-----+------------------------+-----------+---------------+------------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "df.show(1, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- province_state: string (nullable = true)\n",
      " |-- country_region: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      " |-- iso_country_code: string (nullable = true)\n",
      " |-- Region Code: integer (nullable = true)\n",
      " |-- Sub-region Code: integer (nullable = true)\n",
      " |-- Intermediate Region Code: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "// Rename column to allow storing\n",
    "val cleanDf = df.\n",
    "    withColumnRenamed(\"ISO 3166-1 Alpha 3-Codes\", \"iso_country_code\").\n",
    "    withColumnRenamed(\"Province/State\", \"province_state\").\n",
    "    withColumnRenamed(\"Country/Region\", \"country_region\")\n",
    "\n",
    "cleanDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val cleanDf = df.\n",
    "    withColumn(\"date\", unix_timestamp($\"date\", datePattern).cast(\"timestamp\"))\n",
    "//     withColumn(\"timestamp\", unix_timestamp($\"date\", datePattern).cast(\"timestamp\")).\n",
    "//     drop(\"date\")\n",
    "\n",
    "cleanDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+--------+--------+\n",
      "|count(Lat)|         avg(Lat)| stddev_samp(Lat)|min(Lat)|max(Lat)|\n",
      "+----------+-----------------+-----------------+--------+--------+\n",
      "|     67564|21.07662424812158|24.85792699104551|-51.7963| 71.7069|\n",
      "+----------+-----------------+-----------------+--------+--------+"
     ]
    }
   ],
   "source": [
    "// OR df.describe().select(\"summary\", \"Lat\").show()\n",
    "df.agg(count(\"Lat\"), mean(\"Lat\"), stddev(\"Lat\"), min(\"Lat\"), max(\"Lat\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266"
     ]
    }
   ],
   "source": [
    "// Cardinality\n",
    "val distinctValuesDF = cleanDf.\n",
    "    select($\"iso_country_code\", $\"country_region\", $\"province_state\", $\"Lat\", $\"Long\").\n",
    "    distinct\n",
    "print(distinctValuesDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|iso_country_code|count|\n",
      "+----------------+-----+\n",
      "|             CHN|   31|\n",
      "|             CAN|   12|\n",
      "|             AUS|    8|\n",
      "|            null|    5|\n",
      "|             SPM|    1|\n",
      "|             FRA|    1|\n",
      "|             POL|    1|\n",
      "|             TCA|    1|\n",
      "|             LVA|    1|\n",
      "|             JAM|    1|\n",
      "|             ZMB|    1|\n",
      "|             BRA|    1|\n",
      "|             ARM|    1|\n",
      "|             MOZ|    1|\n",
      "|             JOR|    1|\n",
      "|             CUB|    1|\n",
      "|             ABW|    1|\n",
      "|             SOM|    1|\n",
      "|             BRN|    1|\n",
      "|             COD|    1|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "// Find redundancies\n",
    "distinctValuesDF.groupBy(\"iso_country_code\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------------+-------+-------+\n",
      "|iso_country_code|  country_region|  province_state|    Lat|   Long|\n",
      "+----------------+----------------+----------------+-------+-------+\n",
      "|            null|  United Kingdom| Channel Islands|49.3723|-2.3644|\n",
      "|            null|      MS Zaandam|            null|    0.0|    0.0|\n",
      "|            null|Diamond Princess|            null|    0.0|    0.0|\n",
      "|            null|          Canada|Diamond Princess|    0.0|    0.0|\n",
      "|            null|          Canada|  Grand Princess|    0.0|    0.0|\n",
      "+----------------+----------------+----------------+-------+-------+"
     ]
    }
   ],
   "source": [
    "// SELECT IS NULL\n",
    "distinctValuesDF.filter(\"iso_country_code is null\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+--------------+------------------+--------+\n",
      "|iso_country_code|country_region|province_state|               Lat|    Long|\n",
      "+----------------+--------------+--------------+------------------+--------+\n",
      "|             CHN|         China|      Xinjiang|           41.1129| 85.2401|\n",
      "|             CHN|         China|        Shanxi|           37.5777|112.2922|\n",
      "|             CHN|         China|       Guangxi|           23.8298|108.7881|\n",
      "|             CHN|         China|         Hebei|            39.549|116.1306|\n",
      "|             CHN|         China|      Shanghai|31.201999999999998|121.4491|\n",
      "|             CHN|         China|         Hunan|           27.6104|111.7088|\n",
      "|             CHN|         China|      Shandong|           36.3427|118.1498|\n",
      "|             CHN|         China|       Beijing|           40.1824|116.4142|\n",
      "|             CHN|         China|       Shaanxi|           35.1917|108.8701|\n",
      "|             CHN|         China|         Tibet|           31.6927| 88.0924|\n",
      "|             CHN|         China|       Jiangxi|            27.614|115.7221|\n",
      "|             CHN|         China|         Hubei|           30.9756|112.2707|\n",
      "|             CHN|         China|       Guizhou|           26.8154|106.8748|\n",
      "|             CHN|         China|         Anhui|           31.8257|117.2264|\n",
      "|             CHN|         China|  Heilongjiang|47.861999999999995|127.7615|\n",
      "|             CHN|         China|        Fujian|           26.0789|117.9874|\n",
      "|             CHN|         China|         Gansu|           35.7518|104.2861|\n",
      "|             CHN|         China|        Hainan|           19.1959|109.7453|\n",
      "|             CHN|         China|         Henan|           37.8957|114.9042|\n",
      "|             CHN|         China|       Ningxia|           37.2692|106.1655|\n",
      "+----------------+--------------+--------------+------------------+--------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "// SELECT WHERE =\n",
    "distinctValuesDF.filter($\"iso_country_code\" === (\"CHN\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+--------------------+------------------+---------+\n",
      "|iso_country_code|country_region|      province_state|               Lat|     Long|\n",
      "+----------------+--------------+--------------------+------------------+---------+\n",
      "|             USA|            US|                null|              40.0|   -100.0|\n",
      "|             FRA|        France|                null|           46.2276|   2.2137|\n",
      "|             CAN|        Canada|    British Columbia|           53.7267|-127.6476|\n",
      "|             CAN|        Canada|            Manitoba|           53.7609| -98.8139|\n",
      "|             CAN|        Canada|             Ontario|           51.2538| -85.3232|\n",
      "|             CAN|        Canada|        Saskatchewan|           52.9399|-106.4509|\n",
      "|             CAN|        Canada|Newfoundland and ...|           53.1355| -57.6604|\n",
      "|             CAN|        Canada|               Yukon|           64.2823|   -135.0|\n",
      "|             CAN|        Canada|             Alberta|           53.9333|-116.5765|\n",
      "|             CAN|        Canada|       New Brunswick|           46.5653| -66.4619|\n",
      "|             CAN|        Canada|         Nova Scotia|44.681999999999995| -63.7443|\n",
      "|             CAN|        Canada|              Quebec|           52.9399| -73.5491|\n",
      "|             CAN|        Canada|Prince Edward Island|           46.5107| -63.4168|\n",
      "|             CAN|        Canada|Northwest Territo...|           64.8255|-124.8457|\n",
      "|             AUS|     Australia|          Queensland|          -27.4698| 153.0251|\n",
      "|             AUS|     Australia|   Western Australia|          -31.9505| 115.8605|\n",
      "|             AUS|     Australia|Australian Capita...|          -35.4735| 149.0124|\n",
      "|             AUS|     Australia|            Victoria|          -37.8136| 144.9631|\n",
      "|             AUS|     Australia|            Tasmania|          -42.8821| 147.3272|\n",
      "|             AUS|     Australia|     New South Wales|          -33.8688| 151.2093|\n",
      "+----------------+--------------+--------------------+------------------+---------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "// SELECT WHERE IN & ORDER BY\n",
    "distinctValuesDF.filter($\"iso_country_code\" isin (\"CAN\", \"AUS\", \"FRA\", \"USA\")).orderBy(desc(\"iso_country_code\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing - By Time Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res41: Long = 8246"
     ]
    }
   ],
   "source": [
    "// Assess the size first\n",
    "cleanDf.filter(cleanDf(\"Date\").gt(lit(\"2020-09-01\"))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+----------------+---------------------------------------------+----------+\n",
      "|iso_country_code|country_region        |province_state  |window                                       |casualties|\n",
      "+----------------+----------------------+----------------+---------------------------------------------+----------+\n",
      "|BRN             |Brunei                |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|3         |\n",
      "|ZWE             |Zimbabwe              |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|228       |\n",
      "|SYR             |Syria                 |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|202       |\n",
      "|AIA             |United Kingdom        |Anguilla        |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|0         |\n",
      "|LBR             |Liberia               |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|82        |\n",
      "|ESH             |Western Sahara        |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|1         |\n",
      "|BMU             |United Kingdom        |Bermuda         |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|9         |\n",
      "|BIH             |Bosnia and Herzegovina|null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|861       |\n",
      "|GHA             |Ghana                 |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|301       |\n",
      "|CAN             |Canada                |New Brunswick   |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|2         |\n",
      "|TGO             |Togo                  |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|48        |\n",
      "|BFA             |Burkina Faso          |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|58        |\n",
      "|PYF             |France                |French Polynesia|[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|7         |\n",
      "|BLZ             |Belize                |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|27        |\n",
      "|GAB             |Gabon                 |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|54        |\n",
      "|TJK             |Tajikistan            |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|77        |\n",
      "|JPN             |Japan                 |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|1580      |\n",
      "|GNB             |Guinea-Bissau         |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|39        |\n",
      "|CRI             |Costa Rica            |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|917       |\n",
      "|AUT             |Austria               |null            |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|802       |\n",
      "+----------------+----------------------+----------------+---------------------------------------------+----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "// filter data where the date is greater than 2020-09-01\n",
    "val tsDF = cleanDf.filter(cleanDf(\"Date\").gt(lit(\"2020-09-01\"))).\n",
    "    groupBy($\"iso_country_code\", $\"country_region\", $\"province_state\", window($\"Date\", \"6 hours\")).\n",
    "    agg(sum('Value) as \"casualties\").\n",
    "    orderBy(desc(\"window.start\"))\n",
    "\n",
    "tsDF.\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data into Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+--------------+---------------------+---------------------+----------+\n",
      "|iso_country_code|country_region|province_state|start                |end                  |casualties|\n",
      "+----------------+--------------+--------------+---------------------+---------------------+----------+\n",
      "|TJK             |Tajikistan    |unknown       |2020-10-01 00:00:00.0|2020-10-01 06:00:00.0|77        |\n",
      "+----------------+--------------+--------------+---------------------+---------------------+----------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.{SQLContext, SaveMode}\n",
    "\n",
    "// Select ONLY needed columns\n",
    "val storedDF = tsDF.select(\"iso_country_code\", \"country_region\", \"province_state\", \"window.start\", \"window.end\", \"casualties\").\n",
    "    // removing row when some value is null\n",
    "    na.drop(Array(\"iso_country_code\")).\n",
    "    // Replace unknown value with others\n",
    "    na.fill(\"unknown\", Array(\"country_region\", \"province_state\"))\n",
    "\n",
    "storedDF.\n",
    "    write.mode(SaveMode.Overwrite).\n",
    "    saveAsTable(\"data\")\n",
    "\n",
    "// Display one line as sample\n",
    "storedDF.show(1, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataframe to %%local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandaDf: org.apache.spark.sql.DataFrame = [iso_country_code: string, start: timestamp ... 1 more field]"
     ]
    }
   ],
   "source": [
    "%%spark -o pandaDf\n",
    "val pandaDf = tsDF.select(\"iso_country_code\", \"window.start\", \"casualties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700186abb5e54ee9a0c478ba47a0dc87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ae1df238394392811f517a9764702b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "pandaDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+---------+-----------+\n",
      "|name|database|description|tableType|isTemporary|\n",
      "+----+--------+-----------+---------+-----------+\n",
      "|data| default|       null|  MANAGED|      false|\n",
      "+----+--------+-----------+---------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.catalog.listTables(\"default\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL - Data query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4cea5bfa5b4430a8405a35cf53f237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(), EncodingWidget(children=(VBox(children=(HTML(value='Encoding:'), Dropdown(description='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f949b878b174666aba8e85f85114127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "INSERT INTO data VALUES (null, null, null, \"2020-08-29T00:00:00.000Z\", \"2020-08-29T00:00:00.000Z\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffa0e05a8b44a198e9bb80e7ec18992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547d72d4ee354231b86eff20d8eeee86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql -c sql -o pdf\n",
    "SELECT start as time, iso_country_code, casualties \n",
    "FROM data\n",
    "ORDER BY time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac176fdaf57f47e0bf641283afa67003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bae24db22b34ed7b6e7679789aff14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT count(distinct iso_country_code) as distinct_country_code\n",
    "FROM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2adfbfacfee41c18421aaca0e3b05ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee34651d72f408b97be447be55db1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT DAY(start) as day, SUM(casualties) as daily_casualties\n",
    "FROM data\n",
    "WHERE iso_country_code = 'FRA'\n",
    "GROUP BY DAY(start)\n",
    "ORDER BY day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using local python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure matplotlib & panda is install before executing following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// import sys  \n",
    "// !{sys.executable} -m pip install --user matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2500 entries, 0 to 2499\n",
      "Data columns (total 3 columns):\n",
      "time                2500 non-null object\n",
      "iso_country_code    2499 non-null object\n",
      "casualties          2500 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 58.7+ KB\n",
      "None\n",
      "          casualties\n",
      "count    2500.000000\n",
      "mean     3424.409200\n",
      "std     16176.413174\n",
      "min         0.000000\n",
      "25%         5.000000\n",
      "50%        60.000000\n",
      "75%       594.750000\n",
      "max    191766.000000\n",
      "[['2020-08-29T00:00:00.000Z' nan 0]\n",
      " ['2020-09-01T00:00:00.000Z' 'CHN' 2]\n",
      " ['2020-09-01T00:00:00.000Z' 'MSR' 1]\n",
      " ...\n",
      " ['2020-09-10T00:00:00.000Z' 'LKA' 12]\n",
      " ['2020-09-10T00:00:00.000Z' 'ETH' 974]\n",
      " ['2020-09-10T00:00:00.000Z' 'GNB' 39]]\n",
      "RangeIndex(start=0, stop=2500, step=1)\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "print(pdf.info())\n",
    "print(pdf.describe())\n",
    "print(pdf.values)\n",
    "print(pdf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf692e998454abeb3de523b64860014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e541114bc66b4ee685f04d8024bd8cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "import pandas as pd\n",
    "\n",
    "pdf['timestamp'] = pd.to_datetime(pdf[\"time\"])\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time                        object\n",
      "iso_country_code            object\n",
      "casualties                   int64\n",
      "timestamp           datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "print(pdf.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(pdf['time'], pdf['casualties'], color='red')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "fig, ax = plt.subplots()\n",
    "for country in pandaDf['iso_country_code']:\n",
    "    dataframe = pandaDf.loc[pandaDf['iso_country_code'] == country]\n",
    "    ax.plot(dataframe['start'], dataframe['casualties'], label=country)\n",
    "\n",
    "ax.set_title('Casualties per country')\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "\n",
    "sns.catplot(x ='time', y ='casualties', data = pdf)\n",
    "plt.title('COVID-19 Casualities')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
