{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkMagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th>Magic</th>\n",
       "    <th>Example</th>\n",
       "    <th>Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>info</td>\n",
       "    <td>%%info</td>\n",
       "    <td>Outputs session information for the current Livy endpoint.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>cleanup</td>\n",
       "    <td>%%cleanup -f</td>\n",
       "    <td>Deletes all sessions for the current Livy endpoint, including this notebook's session. The force flag is mandatory.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>delete</td>\n",
       "    <td>%%delete -f -s 0</td>\n",
       "    <td>Deletes a session by number for the current Livy endpoint. Cannot delete this kernel's session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>logs</td>\n",
       "    <td>%%logs</td>\n",
       "    <td>Outputs the current session's Livy logs.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure</td>\n",
       "    <td>%%configure -f<br/>{\"executorMemory\": \"1000M\", \"executorCores\": 4}</td>\n",
       "    <td>Configure the session creation parameters. The force flag is mandatory if a session has already been\n",
       "    created and the session will be dropped and recreated.<br/>Look at <a href=\"https://github.com/cloudera/livy#request-body\">\n",
       "    Livy's POST /sessions Request Body</a> for a list of valid parameters. Parameters must be passed in as a JSON string.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>spark</td>\n",
       "    <td>%%spark -o df<br/>df = spark.read.parquet('...</td>\n",
       "    <td>Executes spark commands.\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The Spark dataframe of name VAR_NAME will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe with the same name.</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a dataframe that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>sql</td>\n",
       "    <td>%%sql -o tables -q<br/>SHOW TABLES</td>\n",
       "    <td>Executes a SQL query against the variable sqlContext (Spark v1.x) or spark (Spark v2.x).\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The result of the SQL query will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe.</li>\n",
       "        <li>-q: The magic will return None instead of the dataframe (no visualization).</li>\n",
       "        <li>-m, -n, -r are the same as the %%spark parameters above.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>local</td>\n",
       "    <td>%%local<br/>a = 1</td>\n",
       "    <td>All the code in subsequent lines will be executed locally. Code must be valid Python code.</td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detailed command, please see https://docs.faculty.ai/how_to/spark/external_cluster.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f \n",
    "{\n",
    "    \"name\":\"sparkmagic-demo\",\n",
    "    \"kind\": \"spark\",\n",
    "    \"executorMemory\": \"4G\", \n",
    "    \"executorCores\": 4,\n",
    "    \"driverMemory\": \"1000M\", \n",
    "    \"numExecutors\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "res2: String = 2.1.0"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "// https://spark.apache.org/docs/2.1.0/sql-programming-guide.html\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res4: String = in-memory"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.catalogImplementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.driver.host,172.26.0.2)\n",
      "(spark.livy.spark_major_version,2)\n",
      "(spark.driver.port,40799)\n",
      "(hive.metastore.warehouse.dir,file:/usr/spark-2.1.0/spark-warehouse/)\n",
      "(spark.repl.class.uri,spark://172.26.0.2:40799/classes)\n",
      "(spark.jars,file:/apps/livy-server-0.3.0/rsc-jars/spark-tags_2.10-2.1.0.jar,file:/apps/livy-server-0.3.0/rsc-jars/livy-api-0.3.0.jar,file:/apps/livy-server-0.3.0/rsc-jars/unused-1.0.0.jar,file:/apps/livy-server-0.3.0/rsc-jars/netty-all-4.0.29.Final.jar,file:/apps/livy-server-0.3.0/rsc-jars/livy-rsc-0.3.0.jar,file:/apps/livy-server-0.3.0/repl_2.11-jars/commons-codec-1.9.jar,file:/apps/livy-server-0.3.0/repl_2.11-jars/livy-core_2.11-0.3.0.jar,file:/apps/livy-server-0.3.0/repl_2.11-jars/livy-repl_2.11-0.3.0.jar)\n",
      "(spark.repl.class.outputDir,/tmp/spark7551688344206775368)\n",
      "(spark.app.name,livy-session-0)\n",
      "(spark.driver.memory,1000M)\n",
      "(spark.executor.id,driver)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.yarn.maxAppAttempts,1)\n",
      "(spark.master,local)\n",
      "(spark.yarn.submit.waitAppCompletion,false)\n",
      "(spark.sql.catalogImplementation,in-memory)\n",
      "(spark.executor.cores,2)\n",
      "(spark.app.id,local-1601797694742)"
     ]
    }
   ],
   "source": [
    "spark.conf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.master,local)\n",
      "(spark.repl.class.outputDir,/tmp/spark7551688344206775368)\n",
      "(spark.sql.catalogImplementation,in-memory)\n",
      "(spark.driver.host,172.26.0.2)\n",
      "(spark.yarn.submit.waitAppCompletion,false)\n",
      "(spark.driver.port,40799)\n",
      "(spark.executor.id,driver)\n",
      "(spark.executor.cores,2)\n",
      "(spark.app.name,livy-session-0)\n",
      "(spark.repl.class.uri,spark://172.26.0.2:40799/classes)\n",
      "(spark.jars,file:/apps/livy-server-0.3.0/rsc-jars/spark-tags_2.10-2.1.0.jar,file:/apps/livy-server-0.3.0/rsc-jars/livy-api-0.3.0.jar,file:/apps/livy-server-0.3.0/rsc-jars/unused-1.0.0.jar,file:/apps/livy-server-0.3.0/rsc-jars/netty-all-4.0.29.Final.jar,file:/apps/livy-server-0.3.0/rsc-jars/livy-rsc-0.3.0.jar,file:/apps/livy-server-0.3.0/repl_2.11-jars/commons-codec-1.9.jar,file:/apps/livy-server-0.3.0/repl_2.11-jars/livy-core_2.11-0.3.0.jar,file:/apps/livy-server-0.3.0/repl_2.11-jars/livy-repl_2.11-0.3.0.jar)\n",
      "(spark.yarn.maxAppAttempts,1)\n",
      "(spark.livy.spark_major_version,2)\n",
      "(spark.submit.deployMode,client)\n",
      "(hive.metastore.warehouse.dir,file:/usr/spark-2.1.0/spark-warehouse/)\n",
      "(spark.app.id,local-1601797694742)\n",
      "(spark.driver.memory,1000M)"
     ]
    }
   ],
   "source": [
    "println(sc.version)\n",
    "sc.getConf.getAll.\n",
    "    foreach(println)\n",
    "//     mkString(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Dataset\n",
    "import spark.sqlContext.implicits._\n",
    "\n",
    "val jsonInCSV: Dataset[String] = sc.parallelize(List(\n",
    "  \"\"\"\n",
    "    |\"a\",\"b\",\"c\",\"{\"\"x\"\":\"\"xx\"\",\"\"y\"\":\"\"yy\"\"}\"\n",
    "  \"\"\".stripMargin)).toDS()\n",
    "\n",
    "val df = spark.read.option(\"escape\", \"\\\"\").csv(jsonInCSV)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val value = \"[\\\"1978-08-19T19:08:08.788Z\\\",\\\"2020-08-19T19:08:08.788Z\\\"]\"\n",
    "value.substring(2, value.indexOf(',')-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.text.SimpleDateFormat\n",
    "\n",
    "val pattern = \"dd/MM/yyyy HH:mm\"\n",
    "\n",
    "val format = new SimpleDateFormat(pattern)\n",
    "format.parse(\"01/01/2018 00:00\").getTime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.text.SimpleDateFormat\n",
    "\n",
    "def parseDateUDF(p: String) = udf(\n",
    "  (value: String) => {\n",
    "    val dateFormat = new SimpleDateFormat(p)\n",
    "    val parsedDate = dateFormat.parse(value)\n",
    "    new java.sql.Timestamp(parsedDate.getTime())\n",
    "  }\n",
    ")\n",
    "\n",
    "val pattern = \"dd/MM/yyyy HH:mm\"\n",
    "\n",
    "// Example data\n",
    "val df = Seq(\n",
    "  Tuple1(\"01/01/2018 00:00\")\n",
    ").toDF(\"stringTime\")\n",
    "\n",
    "val newDF = df.withColumn(\"timestamp\", parseDateUDF(pattern)(df(\"stringTime\"))).orderBy(\"timestamp\")\n",
    "newDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [Province/State: string, Country/Region: string ... 8 more fields]"
     ]
    }
   ],
   "source": [
    "// http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframereader#pyspark.sql.DataFrameReader.csv\n",
    "\n",
    "val isoDatePattern = \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"\n",
    "val datePattern = \"yyyy-MM-dd HH:mm:ss.SSS\"\n",
    "\n",
    "val df = spark.read.\n",
    "    option(\"header\", \"true\").\n",
    "    option(\"inferSchema\", \"true\").\n",
    "    option(\"delimiter\", \",\").\n",
    "// Use this line to allow detect Date type\n",
    "//     option(\"timestampFormat\", datePattern).\n",
    "// Use this line if one row can have line break\n",
    "//     option(\"multiline\",true).\n",
    "// Use this line if one column has JSON confuse with delimiter \n",
    "//     option(\"escape\", \"\\\"\").\n",
    "    csv(\"file:///data/time_series_covid19_deaths_global_narrow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Province/State: string (nullable = true)\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      " |-- ISO 3166-1 Alpha 3-Codes: string (nullable = true)\n",
      " |-- Region Code: integer (nullable = true)\n",
      " |-- Sub-region Code: integer (nullable = true)\n",
      " |-- Intermediate Region Code: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------+---------+---------------------+-----+------------------------+-----------+---------------+------------------------+\n",
      "|Province/State|Country/Region|Lat     |Long     |Date                 |Value|ISO 3166-1 Alpha 3-Codes|Region Code|Sub-region Code|Intermediate Region Code|\n",
      "+--------------+--------------+--------+---------+---------------------+-----+------------------------+-----------+---------------+------------------------+\n",
      "|null          |Afghanistan   |33.93911|67.709953|2020-10-01 00:00:00.0|1458 |AFG                     |142        |34             |null                    |\n",
      "+--------------+--------------+--------+---------+---------------------+-----+------------------------+-----------+---------------+------------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "df.show(1, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Province/State: string (nullable = true)\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      " |-- iso_country_code: string (nullable = true)\n",
      " |-- Region Code: integer (nullable = true)\n",
      " |-- Sub-region Code: integer (nullable = true)\n",
      " |-- Intermediate Region Code: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "// Rename column to allow storing\n",
    "val cleanDf = df.withColumnRenamed(\"ISO 3166-1 Alpha 3-Codes\", \"iso_country_code\")\n",
    "cleanDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val cleanDf = df.\n",
    "    withColumn(\"date\", unix_timestamp($\"date\", datePattern).cast(\"timestamp\"))\n",
    "//     withColumn(\"timestamp\", unix_timestamp($\"date\", datePattern).cast(\"timestamp\")).\n",
    "//     drop(\"date\")\n",
    "\n",
    "cleanDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing - By Time Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res18: Long = 8246"
     ]
    }
   ],
   "source": [
    "// Assess the size first\n",
    "cleanDf.filter(cleanDf(\"Date\").gt(lit(\"2020-09-01\"))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------------------------------+----------+\n",
      "|iso_country_code|window                                       |casualties|\n",
      "+----------------+---------------------------------------------+----------+\n",
      "|NGA             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|1112      |\n",
      "|SLE             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|72        |\n",
      "|ESP             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|31973     |\n",
      "|ZWE             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|228       |\n",
      "|HND             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|2380      |\n",
      "|LVA             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|37        |\n",
      "|BOL             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|8001      |\n",
      "|MWI             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|179       |\n",
      "|AGO             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|185       |\n",
      "|LIE             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|1         |\n",
      "|MRT             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|161       |\n",
      "|TLS             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|0         |\n",
      "|MOZ             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|62        |\n",
      "|GAB             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|54        |\n",
      "|SSD             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|49        |\n",
      "|PYF             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|7         |\n",
      "|STP             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|15        |\n",
      "|COD             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|272       |\n",
      "|LUX             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|125       |\n",
      "|ITA             |[2020-10-01 00:00:00.0,2020-10-01 06:00:00.0]|35918     |\n",
      "+----------------+---------------------------------------------+----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "// filter data where the date is greater than 2020-09-01\n",
    "val tsDF = cleanDf.filter(cleanDf(\"Date\").gt(lit(\"2020-09-01\"))).\n",
    "    groupBy($\"iso_country_code\", window($\"Date\", \"6 hours\")).\n",
    "    agg(sum('Value) as \"casualties\").\n",
    "    orderBy(desc(\"window.start\"))\n",
    "\n",
    "tsDF.\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data into Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{SQLContext, SaveMode}\n",
    "\n",
    "tsDF.write.mode(SaveMode.Overwrite).\n",
    "    saveAsTable(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataframe to %%local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandaDf: org.apache.spark.sql.DataFrame = [iso_country_code: string, start: timestamp ... 1 more field]"
     ]
    }
   ],
   "source": [
    "%%spark -o pandaDf\n",
    "val pandaDf = tsDF.select(\"iso_country_code\", \"window.start\", \"casualties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9b3c2e747746e2885f6eabcbb2da13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), stâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71606c0ebd44951b3dcf7a6acecd57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "pandaDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables(\"default\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSQL - Data query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sql -c sql -o pdf\n",
    "SELECT window.start as time, iso_country_code, casualties \n",
    "FROM data\n",
    "ORDER BY time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT DAY(window.start) as day, SUM(casualties) as daily_casualties\n",
    "FROM data\n",
    "WHERE iso_country_code = 'FRA'\n",
    "GROUP BY DAY(window.start)\n",
    "ORDER BY day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using local python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure matplotlib & panda is install before executing following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// import sys  \n",
    "// !{sys.executable} -m pip install --user matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "print(pdf.info())\n",
    "print(pdf.describe())\n",
    "print(pdf.values)\n",
    "print(pdf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "import pandas as pd\n",
    "\n",
    "pdf['timestamp'] = pd.to_datetime(pdf[\"time\"])\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "print(pdf.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.plot(pdf['time'], pdf['casualties'], color='red')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "\n",
    "sns.catplot(x ='time', y ='casualties', data = pdf)\n",
    "plt.title('COVID-19 Casualities')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
